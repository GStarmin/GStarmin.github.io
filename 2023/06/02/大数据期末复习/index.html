

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Starmin">
  <meta name="keywords" content="">
  
    <meta name="description" content="大数据期末复习 Ch1 Intruction What is big data? Big data is used to describe a massive volume of both structured and unstructured data that is so large that it&#39;s difficult to process using traditional databa">
<meta property="og:type" content="article">
<meta property="og:title" content="大数据期末复习">
<meta property="og:url" content="https://gstarmin.github.io/2023/06/02/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/">
<meta property="og:site_name" content="私人杂货铺">
<meta property="og:description" content="大数据期末复习 Ch1 Intruction What is big data? Big data is used to describe a massive volume of both structured and unstructured data that is so large that it&#39;s difficult to process using traditional databa">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/4V-features.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/KDD-process.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch2-lossfunction.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch3-the-big-pic.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch3-similarity.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch3-signature计算方法.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/Partion-Into-Bands.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/Partion-Into-Bands-例子.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/Spectral-Hashing公式.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch3-Learning-Based-Hashing-步骤.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch4-CDF.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch4-Rejection-Sampling.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch4-Rejection-Sampling-步骤.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch4-MCMC流程.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch4-MH算法流程.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch4-MH算法具体流程.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch4-Gibbs.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch4-Gibbs-二维流程.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch4-Gibbs-多维流程.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch4-Reservoir.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch5-CVFDT-流程.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch5-流聚类算法.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch5-CF属性-1.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch6-ratio-cut.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch6-norm-cut.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch6-Modularity-Maximization-例子图.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch6-Distance-Dynamics-流程.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch7-Hadoop-Architecture.png">
<meta property="article:published_time" content="2023-06-02T09:38:02.000Z">
<meta property="article:modified_time" content="2023-06-07T03:30:00.000Z">
<meta property="article:author" content="Starmin">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://gstarmin.github.io/img/大数据/4V-features.png">
  
  
  
  <title>大数据期末复习 - 私人杂货铺</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"gstarmin.github.io","root":"/","version":"1.9.3","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 5.4.2"><style>.mjpage .MJX-monospace {
  font-family: monospace;
}

.mjpage .MJX-sans-serif {
  font-family: sans-serif;
}

.mjpage {
  display: inline;
  font-style: normal;
  font-weight: normal;
  line-height: normal;
  font-size: 100%;
  font-size-adjust: none;
  text-indent: 0;
  text-align: left;
  text-transform: none;
  letter-spacing: normal;
  word-spacing: normal;
  word-wrap: normal;
  white-space: nowrap;
  float: none;
  direction: ltr;
  max-width: none;
  max-height: none;
  min-width: 0;
  min-height: 0;
  border: 0;
  padding: 0;
  margin: 0;
}

.mjpage * {
  transition: none;
  -webkit-transition: none;
  -moz-transition: none;
  -ms-transition: none;
  -o-transition: none;
}

.mjx-svg-href {
  fill: blue;
  stroke: blue;
}

.MathJax_SVG_LineBox {
  display: table !important;
}

.MathJax_SVG_LineBox span {
  display: table-cell !important;
  width: 10000em !important;
  min-width: 0;
  max-width: none;
  padding: 0;
  border: 0;
  margin: 0;
}

.mjpage__block {
  text-align: center;
  margin: 1em 0em;
  position: relative;
  display: block !important;
  text-indent: 0;
  max-width: none;
  max-height: none;
  min-width: 0;
  min-height: 0;
  width: 100%;
}
</style></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Starmin</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="大数据期末复习"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-06-02 17:38" pubdate>
          2023年6月2日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          20k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          203 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">大数据期末复习</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="大数据期末复习">大数据期末复习</h1>
<h2 id="ch1-intruction">Ch1 Intruction</h2>
<h3 id="what-is-big-data">What is big data?</h3>
<p><strong>Big data</strong> is used to describe a massive volume of
both structured and unstructured data that is so large that it's
difficult to process using traditional database and software
techniques.</p>
<p><strong>大数据</strong>用来描述大量的结构化和非结构化的数据，这些数据非常大，难以用传统的数据库和软件技术来处理。</p>
<h3 id="the-4v-features-of-big-data">The 4V Features of big data</h3>
<p><img src="/img/大数据/4V-features.png" srcset="/img/loading.gif" lazyload /></p>
<ul>
<li>Volume (Scale of Data)</li>
<li>Velocity (Data Stream)</li>
<li>Variety (Different types of data)</li>
<li>Veracity (Uncertainty, missing value)</li>
</ul>
<h3 id="what-is-data-mining">What is data mining?</h3>
<p><strong>Data mining</strong> consists of <strong>applying data
analysis</strong> and <strong>discovery algorithms</strong> that, under
acceptable computational efficiency limitations, produce a particular
enumeration of patterns over the data.</p>
<p><strong>数据挖掘</strong>包括<strong>应用数据分析</strong>和<strong>发现算法</strong>，在可接受的计算效率限制下，在数据上产生特定的模式列举。</p>
<h3 id="the-kdd-processcore-part">The KDD Process(core part)</h3>
<p><img src="/img/大数据/KDD-process.png" srcset="/img/loading.gif" lazyload /></p>
<h3 id="the-main-tasks-of-data-mining">The main tasks of Data
mining</h3>
<ul>
<li>Association Rule Mining(关联规则挖掘)</li>
<li>Cluster Analysis(聚类分析)</li>
<li>Classification/Prediction(分类/预测)</li>
<li>Outlier Detection(异常点检测)</li>
</ul>
<h3
id="the-relationship-between-data-minning-and-other-subjectse.g.-database">The
relationship between Data minning and other subjects(e.g. Database)</h3>
<p>Data mining is known as Knowledge Discovery in Database (KDD) in the
field of artificial intelligence, is also considered as a fundamental
step in the process of knowledge discovery in database.</p>
<p>数据挖掘在人工智能领域被称为数据库知识发现（KDD），也被认为是数据库知识发现过程中的一个基本步骤。</p>
<h3 id="the-challenges-of-big-data-mining">The challenges of big data
mining</h3>
<ul>
<li>Curse of dimensionality(维度灾难)</li>
<li>Storage cost</li>
<li>Query speed <!-- - Data Quality
- Data Complexity
- Data Privacy and Security
- Scalability
- Ethics(伦理学)
- interpretability --></li>
</ul>
<h2 id="ch2-foundations-of-data-mining">Ch2 Foundations of Data
Mining</h2>
<h3
id="supervised-learningunsupervised-learningsemi-supervised-learning">Supervised
learning/Unsupervised learning/Semi-supervised learning</h3>
<ul>
<li><strong>Supervised learning</strong>: targets to learn the mapping
function or relationship between the features and the labels based on
the labeled data. Namely, <span class="math inline">\(𝑌=𝐹(𝑋|𝜃)\)</span>.
(e.g. Classification, Prediction)</li>
<li><strong>Unsupervised learning</strong>: aims at learning the
intrinsic structure from unlabeled data. (e.g. Clustering, Latent Factor
Learning and Frequent Items Mining)</li>
<li><strong>Semi-supervised learning</strong>: can be regarded as the
unsupervised learning with some constraints on labels, or the supervised
learning with additional information on the distribution of data.</li>
</ul>
<h3 id="loss-function">LOSS FUNCTION</h3>
<p><img src="/img/大数据/ch2-lossfunction.png" srcset="/img/loading.gif" lazyload /></p>
<p><span class="math inline">\(l_1\)</span> norm:</p>
<p><span class="math display">\[L(\beta) =\frac{1}{N}\sum\limits_{i =
1}^{N}L(Y_i, F(X_i | \beta)) + \frac{\lambda}{2} || \beta
||_2\]</span></p>
<p><span class="math inline">\(l_2\)</span> norm:</p>
<p><span class="math display">\[L(\beta) =\frac{1}{N}\sum\limits_{i =
1}^{N}L(Y_i, F(X_i | \beta)) + \frac{\lambda}{2} || \beta
||_1\]</span></p>
<p><span class="math inline">\(||A||_∗\)</span> nuclear norm:</p>
<p><span class="math display">\[||A||_∗=∑\limits_i{σ_i}(A).\]</span></p>
<h3 id="overfittingunderfitting-problem">Overfitting/Underfitting
problem</h3>
<p><strong>Reason</strong>?</p>
<p><strong>How to avoid overfitting</strong>?</p>
<h3 id="classfied-algorithms">Classfied Algorithms</h3>
<h4 id="decision-tree">Decision Tree</h4>
<ul>
<li>How to construct DT?</li>
<li>Attribute selection Criteria
<ul>
<li>Information Gain</li>
<li>Information Gain Ratio</li>
<li>Gini index</li>
</ul></li>
</ul>
<h4 id="knn">KNN</h4>
<p><strong>Lazy Learning</strong>: Lazy Learning does not extract rules
or generalizations from a specific model. Instead, it searches for
historical instances that are similar to the testing instance and makes
a prediction based on their output results. (Lazy
Learning并没有从特定的模型中提取基本规则或一般情况，而是在预测时查找与测试实例相似的历史实例，并根据它们的输出结果做出预测)</p>
<p><strong>advantage</strong>:</p>
<ul>
<li>local data distribution（适用本地数据分布）</li>
<li>Incremental/online learning（渐进式/在线学习）</li>
<li>large number of classes（可以对很大的类型数量分类）</li>
</ul>
<p><strong>disvantage</strong>:</p>
<ul>
<li>parameter k(要设置参数k)</li>
<li>imbalanced data（数据不平衡时分类效果差）</li>
<li>slow inference（推理慢）</li>
</ul>
<h4 id="naive-bayse">Naive bayse</h4>
<p><strong>basic idea</strong></p>
<p><strong>advantage</strong></p>
<h4 id="svm">SVM</h4>
<ol type="1">
<li>basic concept</li>
<li>Linear seperation problem
<ol type="1">
<li>Why SVM works well on small size of samples?</li>
<li>Good generalization</li>
</ol></li>
<li>NonLinear problem
<ol type="1">
<li>solution: map data into high dimension space</li>
<li>Trick: kernel Trick <span class="math inline">\(K(X,Y) = \Phi(X)
\Phi(Y)\)</span></li>
<li>Kernel function: Gaussian kernel, polynormial kernel</li>
</ol></li>
</ol>
<p>损失函数是平方损失加上L1正则化</p>
<p><span class="math display">\[J(\theta) = \frac{1}{2m} \sum_{i=1}^m
(h_\theta(x^{(i)}) - y^{(i)})^2 + \alpha \sum_{j=1}^n
|\theta_j|\]</span></p>
<p>其中，第一项是平方损失，第二项是L1正则化项，<span
class="math inline">\(\alpha\)</span>是正则化参数。</p>
<h3 id="ensemble-learning">Ensemble Learning</h3>
<p>Rationale for Ensemble Learning: No Free Lunch thm: There is no
algorithm that is always the most accurate</p>
<p><strong>Two Criteria</strong>:</p>
<ul>
<li>Good base learner</li>
<li>diversity</li>
</ul>
<p><strong>Three Strategies</strong>:</p>
<ul>
<li>Bagging(Random Forest)</li>
<li>Boosting(AdaBoost)</li>
<li>Stacking</li>
</ul>
<h3 id="clustering">Clustering</h3>
<h4 id="k-means-procedure-and-darwbacks">K-means procedure and
darwbacks</h4>
<p>K-means procedure:</p>
<ol type="1">
<li>从数据中选择k个对象作为初始聚类中心;</li>
<li>计算每个聚类对象到聚类中心的距离来划分；</li>
<li>再次计算每个聚类中心;</li>
<li>计算标准测度函数，之道达到最大迭代次数，则停止，否则，继续操作。</li>
</ol>
<p>优点：</p>
<ul>
<li>原理简单，实现容易；</li>
<li>复杂度与样本数量线性相关，对于处理大数据集合，该算法非常高效，且伸缩性较好。</li>
</ul>
<p>drawbacks（缺点）:</p>
<ul>
<li>K需要事先给定；</li>
<li>Kmeans需要人为地确定初始聚类中心，不同的初始聚类中心可能导致完全不同的聚类结果；</li>
<li>结果不一定是全局最优，只能保证局部最优；</li>
<li>对噪声和离群点敏感；</li>
<li>该方法不适于发现非凸面形状的簇或大小差别很大的簇；</li>
<li>需样本存在均值（限定数据种类）。</li>
</ul>
<h4 id="dbscan">DBSCAN</h4>
<p><strong>Advantage</strong>:</p>
<ul>
<li>可以对任意形状的稠密数据集进行聚类，相对的，K-Means之类的聚类算法一般只适用于凸数据集。</li>
<li>可以在聚类的同时发现异常点，对数据集中的异常点不敏感;</li>
<li>聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。</li>
</ul>
<p><strong>Disadvantage</strong>:</p>
<ul>
<li>如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差。If the
density of the sample set is not uniform and the distance between
clusters is very different, the clustering quality is poor.</li>
<li>如果样本集较大时，聚类收敛时间较长。If the sample set is large, the
clustering convergence time is long.</li>
<li>调参相对于传统的K-Means之类的聚类算法稍复杂. Parameter adjustment is
more complex than K-means.</li>
</ul>
<h3 id="subspace-learning">Subspace learning</h3>
<h4 id="dimension-reduction">Dimension Reduction</h4>
<p><strong>Models</strong>:</p>
<ul>
<li>Linear methods
<ul>
<li>PCA(Principal Component Analysis)</li>
<li>MDS(Multi-Dimensional Scaling)</li>
</ul></li>
<li>Nonlinear methods
<ul>
<li>LLE(Locally Linear Embedding)</li>
<li>LEM(Laplacian eigenmaps)</li>
<li>Isomap</li>
</ul></li>
</ul>
<h4 id="feature-selectionclassification">Feature
selection(Classification)</h4>
<ul>
<li>Filter Method(IG, <span class="math inline">\(\mathcal{X}^2\)</span>
)</li>
<li>wrapper Method</li>
<li>Embedded Methods</li>
</ul>
<h3 id="subspace-clustering子空间聚类">Subspace
Clustering(子空间聚类)</h3>
<ul>
<li>Sparse subspace clustering (SSC)</li>
<li>Low-rank representation (LRR)</li>
</ul>
<h2 id="ch3-hashing">Ch3 Hashing</h2>
<h3 id="the-role-of-hashing作用">The role of Hashing(作用)</h3>
<ul>
<li>After using the hash code to represent the data, <strong>the
required storage space will be greatly
reduced</strong>（使用哈希码表示数据后，所需要的存储空间会被大幅减小）</li>
<li>Can <strong>reduce data dimensionality</strong>, thereby alleviating
the <strong>dimensionality curse
problem</strong>(可以降低数据维度，从而减轻维度灾难问题)</li>
<li>Can realize fast neighbor retrieval at constant or sub-linear level,
and provide support for the rapid realization of upper-level learning
tasks(可以实现常数或者次线性级别的快速近邻检索，为上层学习任务的快速实现提供支撑)</li>
</ul>
<h3 id="find-similar-items">Find similar items</h3>
<p>Three Essential Techniques for Similar items:</p>
<ul>
<li><strong>K-Shingling</strong>：convert documents, emails, etc., to
sets.</li>
<li><strong>Min-hashing</strong>：convert large sets to short
signatures, while preserving similarity.</li>
<li><strong>Locality-sensitive hashing</strong> : focus on pairs of
signatures likely to be similar.</li>
</ul>
<p><img src="/img/大数据/ch3-the-big-pic.png" srcset="/img/loading.gif" lazyload /></p>
<h4 id="shingles">Shingles</h4>
<p>A k-shingle (or k-gram) for a document is <strong>a sequence of k
characters that appears in the document</strong>.
一个文件的k-shingle（或k-gram）是一个出现在文件中的k个字符的序列。</p>
<p>Example: <span class="math inline">\(k=2; doc = abcab\)</span>. Set
of 2-shingles = <span class="math inline">\(\{ab, bc, ca\}\)</span>.</p>
<h4 id="min-hashing">min-hashing</h4>
<p>definition:min-hash is an algorithm for <strong>text and data
similarity comparison</strong> that efficiently extracts the signature
of each data from large-scale data, thus supporting fast comparison of
their similarity.
min-hash是一种用于文本和数据相似度比较的算法，它可以高效地从大规模数据中提取每个数据的签名，从而支持快速地比较它们之间的相似程度。</p>
<h5 id="signature-matrix-rightarrow-how-to-compute-similarity">signature
matrix <span class="math inline">\(\Rightarrow\)</span> how to compute
similarity</h5>
<p><img src="/img/大数据/ch3-similarity.png" srcset="/img/loading.gif" lazyload /></p>
<p>Jaccard similarity：不将<span
class="math inline">\((0,0)\)</span>计入分母，相同的行占全部行的比率</p>
<p>matrix similarity：相同的行占全部行的比率</p>
<p><strong>Signature Matrix的计算方法</strong>：</p>
<p><img src="/img/大数据/ch3-signature计算方法.png" srcset="/img/loading.gif" lazyload /></p>
<p>上图中间矩阵是输入矩阵，左侧的每一列都代表输入矩阵行的一种排列，那么signature
matrix的每一行都对应左侧的一种排列方式，该行的每列数字对应该种排列方式对应列的第一个1的出现行数。</p>
<h5 id="locality-sensitive-hashinglsh">Locality-Sensitive
Hashing（LSH）</h5>
<p>假设我们在主内存中有代表大量对象的数据</p>
<ul>
<li>可能是对象本身</li>
<li>可能是min-hashing中的签名</li>
</ul>
<p>我们要逐一进行比较，找到那些足够相似的pair。但是检查所有的pair是很困难的。</p>
<ul>
<li>一般的想法：
使用一个函数f(x,y)，告诉人们x和y是否是一个候选对：一对元素的相似性必须被评估。</li>
<li>对于min-hash矩阵：
哈希列到许多桶中，并使同一桶中的元素成为候选对。</li>
</ul>
<p>基本思想：Generate from the collection of all elements (signatures in
our example) a small list of candidate pairs: pairs of elements whose
similarity must be evaluated.</p>
<p>简单来说就是从我们Min-hashing得到的标记矩阵生成可能相似的文档对列表。</p>
<p>候选相似文档对 <span class="math inline">\(\Rightarrow\)</span>
这一对的Jaccard相似度必须被准确计算出来</p>
<p>方法：</p>
<ul>
<li>选一个相似度标准 <span class="math inline">\(t\)</span>，并且 <span
class="math inline">\(t&lt;1\)</span>，如果两个文档的相似度大于 <span
class="math inline">\(t\)</span>，则认为这两个文档相似。</li>
<li>如果列<span class="math inline">\(c\)</span>和列<span
class="math inline">\(d\)</span>被视为候选文档对，那么他们一定要满足
<span
class="math inline">\(M(i,c)=M(i,d)&gt;=t\)</span>，其中M是标记矩阵。</li>
</ul>
<h5 id="lsh-for-minhashing-signatures">LSH for Minhashing
Signatures</h5>
<p>总体思想：把标记矩阵里的hash很多遍，只有hash到同一个桶(bucket)里的列才被认为是可能相似的。</p>
<p><strong>Partion Into Bands</strong></p>
<p><img src="/img/大数据/Partion-Into-Bands.png" srcset="/img/loading.gif" lazyload /></p>
<p>Divide matrix M into b bands of r rows. For each band, hash its
portion of each column to a hash table with k buckets.
如图所示，把标记矩阵(signature matrix)的所有行分成 <span
class="math inline">\(b\)</span> 个带(bands)，每个带有 <span
class="math inline">\(r\)</span>
行。对于每条带，对带里面每列进行hash，分别hash到<span
class="math inline">\(k\)</span>个桶中，并让<span
class="math inline">\(k\)</span>尽可能得大。</p>
<p>只有有<span
class="math inline">\(&gt;=1\)</span>的band哈希到同一个桶中，就把这两列当作候选相似对。</p>
<p><img src="/img/大数据/Partion-Into-Bands-例子.png" srcset="/img/loading.gif" lazyload /></p>
<h5 id="example---bands">Example - Bands</h5>
<p>假设有 100,000 列，每列有100个标记，因此存储标记需要40MB;
我们希望找到所以相似度大于80%的文档对，用上面的方法，把标记分为20个带，每个带里有5个标记。</p>
<p>这样的话，如果文档<span class="math inline">\(C_1\)</span>和<span
class="math inline">\(C_2\)</span>的相似度是<span
class="math inline">\(80\%\)</span>，那么他们的任意一个带的<span
class="math inline">\(5\)</span>个标记都相同的概率是: <span
class="math inline">\((0.8)^5=0.328\)</span>
，看起来好像不大，但是只要有任意一个带都相同就被认为是候选对，所以他们不被选上的概率，即20个带都不相同的概率为：<span
class="math inline">\((1−0.328)^20=0.00035\)</span> ，也就是每<span
class="math inline">\(3000\)</span>个相似度为<span
class="math inline">\(80\%\)</span>的文档对里才会有一对漏选。</p>
<p>我们再考虑文档<span class="math inline">\(C_1\)</span>和<span
class="math inline">\(C_2\)</span>只有<span
class="math inline">\(40\%\)</span>的相似度，那么他们任意一个带的<span
class="math inline">\(5\)</span>个标记都相同的概率为 <span
class="math inline">\((0.4)^5=0.01\)</span>，则文档<span
class="math inline">\(C_1\)</span>和<span
class="math inline">\(C_2\)</span>被选为候选对的概率，即他们中有一个带完全相同的概率为:
<span class="math inline">\(C^1_{20}×0.01=0.2\)</span> ，就是说每<span
class="math inline">\(5\)</span>个<span
class="math inline">\(40\%\)</span>相似度的文档对里就有一对会被误选为候选对。但是相似度小于<span
class="math inline">\(40\%\)</span>的文档对里误选的概率就非常小了。</p>
<h3 id="learn-to-hash">Learn to Hash</h3>
<ol type="1">
<li>Data indenpendent:Random projection</li>
<li>Data dependent:
<ol type="1">
<li>PCA hashing</li>
<li>Spectral Hashing</li>
</ol></li>
</ol>
<h4 id="pca-hashing">PCA hashing</h4>
<p>分为两个阶段</p>
<p><strong>Projection Stage（投影阶段）</strong>:</p>
<p>用一个转换矩阵<span class="math inline">\(W\)</span>,可以将<span
class="math inline">\(x\)</span>投影到一个新的特征平面。</p>
<p><span class="math display">\[Y=W^T X\]</span></p>
<p><strong>Quantization Stage（量化阶段）</strong>:</p>
<p><span class="math display">\[h(x) = sgn(W^T X)\]</span></p>
<p>最小化quantization loss（量化损失）</p>
<p><span class="math display">\[Q(B,Y) = ||B - Y^T R||^2_F\]</span></p>
<p><span class="math inline">\(R\)</span>是正交矩阵. <span
class="math inline">\(B = Sgn(Y^T R)\)</span></p>
<p>基本思想是旋转数据以最小化量化损失。</p>
<p>实现方法：从<span
class="math inline">\(R\)</span>的随机初始化开始，采用类似K-means的迭代算法来优化<span
class="math inline">\(R\)</span>。在每次迭代中，每个数据点首先被分配到最近的聚类中心，然后更新<span
class="math inline">\(R\)</span>以使量化损失最小化。</p>
<h4 id="spectral-hashing谱哈希">Spectral Hashing（谱哈希）</h4>
<p><img src="/img/大数据/Spectral-Hashing公式.png" srcset="/img/loading.gif" lazyload /></p>
<h4
id="general-approach-to-learning-based-hashinglearning-based哈希的一般方法">General
Approach to Learning-Based Hashing(Learning-Based哈希的一般方法)</h4>
<p>将哈希学习问题分解为两个步骤：</p>
<ol type="1">
<li>hash bit learning. 哈希比特学习</li>
<li>hash function learning based on the learned bits.
基于所学习的哈希比特的哈希函数学习</li>
</ol>
<p><img src="/img/大数据/ch3-Learning-Based-Hashing-步骤.png" srcset="/img/loading.gif" lazyload /></p>
<h2 id="ch4-sampling">Ch4 Sampling</h2>
<p>Why sampling?</p>
<ul>
<li>Big data issue
<ul>
<li>Store complexity</li>
<li>Calculate complexity</li>
</ul></li>
<li>Posterior estimation
<ul>
<li>Expectation estimation</li>
</ul></li>
</ul>
<h3 id="inverse-transform-sampling逆采样变换">Inverse Transform
Sampling(逆采样变换)</h3>
<p>Inverse Transform Sampling based on the inverse of Cumulative
Distribution Function (CDF). 逆采样变换（Inverse Transform
Sampling）是伪随机数采样的一种基本方法。在已知任意概率分布的累积分布函数<span
class="math inline">\(CDF\)</span>时，可以通过<span
class="math inline">\(CDF\)</span>的逆函数来实现随机数的采样。</p>
<p>简单来说，假设<span
class="math inline">\(X\)</span>为一个连续随机变量，其概率密度函数为<span
class="math inline">\(PDF(X)\)</span>，累计分布函数为<span
class="math inline">\(CDF(X)\)</span>。这时候若想生成符合<span
class="math inline">\(X\)</span>分布的随机变量样本，只需在<span
class="math inline">\([0, 1]\)</span>范围内生成随机变量<span
class="math inline">\(x\)</span>,然后放入<span
class="math inline">\(CDF\)</span>的反函数中，即可得到符合<span
class="math inline">\(X\)</span>分布的随机变量样本。</p>
<p>方法：</p>
<p><img src="/img/大数据/ch4-CDF.png" srcset="/img/loading.gif" lazyload /></p>
<p>优点：</p>
<ul>
<li>简单</li>
<li>适用于任意分布</li>
</ul>
<p>缺点：</p>
<ul>
<li>Hard to get the inverse function. 很难确定逆函数</li>
</ul>
<h3 id="rejection-sampling拒绝采样">Rejection Sampling(拒绝采样)</h3>
<p>Rejection Sampling accept the samples in the region under the graph
of its density function and reject others. 拒绝采样（Rejection
Sampling）是一种基本的随机数采样方法。它的基本思想是：对于一个难以采样的分布，我们可以找到一个容易采样的分布，使得容易采样的分布包含难以采样的分布，然后从容易采样的分布中采样，若采样的点在难以采样的分布中，则接受该点，否则拒绝该点。</p>
<p>方法：</p>
<p><img src="/img/大数据/ch4-Rejection-Sampling.png" srcset="/img/loading.gif" lazyload /></p>
<p>步骤：</p>
<p><img src="/img/大数据/ch4-Rejection-Sampling-步骤.png" srcset="/img/loading.gif" lazyload /></p>
<p>这就使得Proposal Distribution <span
class="math inline">\(q(x)\)</span>的支撑集（support）要大于目标分布<span
class="math inline">\(p(x)\)</span>的支撑集。所以<span
class="math inline">\(q(x)\)</span>的分布选择很重要。</p>
<h3 id="importance-sampling重要性采样">Importance
Sampling(重要性采样)</h3>
<p>Importance Sampling not reject but assign weight to each instance so
that the correct distribution is targeted. 重要性采样（Importance
Sampling）与Rejection
Sampling(拒绝采样)的区别在于，重要性采样不会拒绝采样的点，而是对采样的点赋予一个权重，使得采样的点更多地来自于目标分布。</p>
<h3 id="importance-sampling和rejection-sampling的区别">Importance
Sampling和Rejection Sampling的区别</h3>
<ul>
<li>RS的实例有一个相同的权重，只有部分的实例会被保留</li>
<li>IS的实例有不同的权重，所有的实例都会被保留</li>
<li>IS对对proposal distribution的选择更不敏感</li>
</ul>
<h3 id="markov-chain-monte-carlomcmc">Markov Chain Monte
Carlo(MCMC)</h3>
<p>MCMC methods are a class of algorithms for <strong>sampling from a
probability distribution based on constructing a Markov chain</strong>
that has the desired distribution as its <strong>equilibrium
distribution</strong>. The state of the chain after a number of steps is
then used as a sample of the desired distribution.</p>
<p>马尔可夫链蒙特卡洛（Markov Chain Monte
Carlo，MCMC）是一种基于马尔可夫链的随机采样方法。它的基本思想是：对于一个难以采样的分布，我们可以构造一个马尔可夫链，使得该马尔可夫链的平稳分布为该难以采样的分布，然后从该马尔可夫链中采样，得到的样本服从该难以采样的分布。</p>
<p><strong>蒙特卡洛法</strong>：</p>
<p>蒙特卡洛法（Monte Carlo
Method）是一种基于随机数的数值计算方法。它的基本思想是：对于一个难以计算的问题，我们可以构造一个概率分布，使得该概率分布的期望为该问题的解，然后从该概率分布中采样，得到的样本的平均值即为该问题的解。</p>
<h4 id="detailed-balance-condition细致平衡条件">Detailed Balance
Condition(细致平衡条件)</h4>
<p>细致平衡条件（Detailed Balance
Condition）是马尔可夫链平稳分布的一个必要条件。它的基本思想是：对于一个马尔可夫链，若该马尔可夫链的平稳分布为<span
class="math inline">\(\pi(x)\)</span>，则该马尔可夫链的任意两个状态<span
class="math inline">\(x\)</span>和<span
class="math inline">\(y\)</span>满足：</p>
<p><span class="math display">\[\pi(x)P(x, y) = \pi(y)P(y,
x)\]</span></p>
<p>其中<span class="math inline">\(P(x, y)\)</span>为从状态<span
class="math inline">\(x\)</span>转移到状态<span
class="math inline">\(y\)</span>的概率,<span
class="math inline">\(\pi(x)\)</span>为状态<span
class="math inline">\(x\)</span>的概率。那么<span
class="math inline">\(\pi(x)\)</span>就是该马尔可夫链的平稳分布。</p>
<h4 id="the-procedure-of-mcmcmcmc的流程">The Procedure of
MCMC(MCMC的流程)</h4>
<p><img src="/img/大数据/ch4-MCMC流程.png" srcset="/img/loading.gif" lazyload /></p>
<h3 id="metropolis-hastings-algorithmmh算法">Metropolis-Hastings
Algorithm(MH算法)</h3>
<p>由于MCMC采样有收敛太慢的问题,所以在MCMC的基础之上进行改进，引出MH算法。</p>
<p><img src="/img/大数据/ch4-MH算法流程.png" srcset="/img/loading.gif" lazyload /></p>
<p>MH算法的具体流程如下：</p>
<p><img src="/img/大数据/ch4-MH算法具体流程.png" srcset="/img/loading.gif" lazyload /></p>
<p>一般来说M-H采样算法较MCMC算法应用更广泛，然而在大数据时代，M-H算法面临着两个问题：</p>
<ol type="1">
<li>在高维时的计算量很大，算法效率很低，同时存在拒绝转移的问题，也会加大计算量</li>
<li>由于特征维度大，很多时候我们甚至很难求出目标的各特征维度联合分布，但是可以方便求出各个特征之间的条件概率分布（因此就思考是否能只知道条件概率分布的情况下进行采样）。</li>
</ol>
<h3 id="gibbs-sampling">Gibbs Sampling</h3>
<p>Gibbs Sampling是MH算法的一种特殊情况。它的基本思想是：</p>
<p><img src="/img/大数据/ch4-Gibbs.png" srcset="/img/loading.gif" lazyload /> <img
src="/img/大数据/ch4-Gibbs-1.png" srcset="/img/loading.gif" lazyload /></p>
<p>因此可以得出在二维的情况下Gibbs采样算法的流程如下：</p>
<p><img src="/img/大数据/ch4-Gibbs-二维流程.png" srcset="/img/loading.gif" lazyload /></p>
<p>而在多维的情况下，比如一个n维的概率分布<span
class="math inline">\(π(x_1, x_2, ...x_n)\)</span>，我们可以通过在<span
class="math inline">\(n\)</span>个坐标轴上轮换采样，来得到新的样本。对于轮换到的任意一个坐标轴<span
class="math inline">\(x_i\)</span>上的转移，马尔科夫链的状态转移概率为<span
class="math inline">\(P(x_i|x_1, x_2, ..., x_{i−1}, x_{i+1}, ...,
x_n)\)</span>，即固定<span
class="math inline">\(n−1\)</span>个坐标轴，在某一个坐标轴上移动。而在多维的情况下Gibbs采样算法的流程如下：</p>
<p><img src="/img/大数据/ch4-Gibbs-多维流程.png" srcset="/img/loading.gif" lazyload /></p>
<h3 id="gibbs-sampling和mh算法的联系与区别">Gibbs
Sampling和MH算法的联系与区别</h3>
<ul>
<li>Gibbs Sampling和MH都是MCMC</li>
<li>Acceptance Ratio:
<ul>
<li>Gibbs Sampling: <span class="math inline">\(1\)</span></li>
<li>MH: <span class="math inline">\(\frac{\pi(y)q(y, x)}{\pi(x)q(x, y)}
&lt; 1\)</span></li>
</ul></li>
<li>MH不需要知道条件概率，而Gibbs Sampling需要知道条件概率</li>
</ul>
<h3 id="reservoir-sampling">Reservoir sampling</h3>
<p>Reservoir
sampling是一种随机采样算法，它的基本思想是：对于一个数据流，我们希望从中随机采样出<span
class="math inline">\(k\)</span>个样本，但是我们不知道数据流的长度，也就是说我们不知道<span
class="math inline">\(k\)</span>的大小。具体流程如下：</p>
<p><img src="/img/大数据/ch4-Reservoir.png" srcset="/img/loading.gif" lazyload /></p>
<h2 id="ch5-data-stream-mining">Ch5 Data Stream Mining</h2>
<h3 id="data-stream">Data Stream</h3>
<p><strong>What is Data Stream?</strong></p>
<p>A data stream is a massive sequence of data objects which have some
unique features.</p>
<h4 id="properity-of-data-stream">Properity of Data Stream</h4>
<p>数据流（Data Stream）是一种连续不断的数据，它的特点是：</p>
<ul>
<li>One by One(逐个到达)</li>
<li>Potentially Unbounded(无界)</li>
<li><strong>Concept Drift(概念漂移)</strong></li>
</ul>
<h4 id="conncept-drift概念漂移">Conncept Drift(概念漂移)</h4>
<p>Concept Drift is the probability distribution changes.</p>
<p>概念漂移（Concept
Drift）是指数据流中的数据分布随着时间的推移而发生变化的现象。概念漂移分为两种类型：</p>
<ul>
<li>Real concept drift (真实概念漂移)</li>
<li>Virtual concept drift (虚假概念漂移)</li>
</ul>
<h4 id="concept-drift-detection概念漂移检测">Concept Drift
Detection(概念漂移检测)</h4>
<ol type="1">
<li>Distribution-based detector(基于分布的检测器)</li>
</ol>
<p>监测两个固定或可变化的数据窗口之间的数据分布变化，如果数据分布发生变化，则认为发生了概念漂移。方法很简单：只要<span
class="math inline">\(W\)</span>的两个足够大的子窗口<span
class="math inline">\(W_1\)</span>和<span
class="math inline">\(W_2\)</span>的数据分布不同，就认为发生了概念漂移，这时较旧的窗口就被放弃。</p>
<p><strong>draw back(缺点)</strong>：</p>
<ul>
<li>Hard to determine window size.</li>
<li>Learn concept drift slower</li>
<li>Virtual concept drift</li>
</ul>
<p><strong>Adaptive Windowing(ADWIN)</strong>:</p>
<p>ADWIN 的思想是从时间窗口 <span class="math inline">\(W\)</span>
开始，在上下文没有明显变化时动态增大窗口 <span
class="math inline">\(W\)</span>，并在检测到变化时将其缩小。
该算法试图找到显示不同平均值的 <span class="math inline">\(W -
w_0\)</span> 和 <span class="math inline">\(w_1\)</span> 的两个子窗口。
这意味着窗口的旧部分 <span class="math inline">\(- w_0\)</span>
是基于与实际不同的数据分布，因此被删除。</p>
<ol type="1">
<li>Error-rate based detector(基于错误率的检测器)</li>
</ol>
<p>根据分类性能的变化来捕捉概念的漂移,如果分类器的错误率超过了某个阈值，则认为发生了概念漂移。</p>
<p>DDM算法：</p>
<p>DDM算法的基本思想是：在数据流中，如果某个时间点的错误率比之前的错误率大很多，则认为发生了概念漂移。确定错误率的变化是否显著的方法如下公式：</p>
<p><span class="math display">\[p_i + s_i \ge p_{min} + 3 \times
s_{min}\]</span></p>
<p>误差率是指观察到错误的概率<span
class="math inline">\(p_i\)</span>，其标准差为<span
class="math inline">\(s_i = sqrt(p_i (1 - p_i) / i)\)</span></p>
<p><strong>draw back(缺点)</strong>：</p>
<ul>
<li>Sensitive to noise</li>
<li>Hard to deal with gradual concept drift</li>
<li>Depend on learning model itself heavily</li>
</ul>
<h4 id="data-stream面临的挑战">Data Stream面临的挑战</h4>
<ul>
<li>Infinite Length(无限长度)</li>
<li>Evolving Nature(不断变化的数据)</li>
</ul>
<h3 id="data-stream-clissification数据流分类">Data Stream
Clissification(数据流分类)</h3>
<p>流程：</p>
<ul>
<li>从数据流中读取下一个可用数据(要求1)</li>
<li>用读取的数据更新分类器，并且这样做不回超过对它设置的内存限制(要求2)，并尽可能快地完成（要求3）</li>
<li>算法已经学习了足够的数据，以便在新数据上进行分类(要求4)</li>
</ul>
<p><strong>典型算法</strong>：</p>
<ul>
<li>VFDT(very fast decision tree, KDD'00)</li>
<li>CVFDT(Concept-adapting very fast decision tree, KDD'01)</li>
<li>SyncStream(同步流算法, KDD'14)</li>
</ul>
<h4 id="vfdt">VFDT</h4>
<p>Hoeffding树是一种基于决策树学习的数据流分类算法，在处理数据流时，可以保证挖掘效率的同时，达到对数据流一些必要操作的要求。该算法简单的对数据流中的每个样本检查一次，并逐步生成一颗决策树，而在这些样本更新完决策树之后无需进行存储。
在内存中只需维护决策树信息，因为在决策树的叶结点中存储着决策树扩展所必须的统计信息，并且在处理训练数据集时，可以用决策树中的信息进行预测。</p>
<p>VFDT（very fast decision tree）是基于Hoeffding
tree改进的算法和系统，它和Hoeffding
tree算法相似之处在于都是根据<strong>Hoeffding
不等式</strong>来决定决策节点的最佳属性从而建立决策树模型。</p>
<p>Hoeffding 不等式:</p>
<p>Hoeffding不等式适用于有界的随机变量。设有两两独立的一系列随机变量<span
class="math inline">\(X_1, X_2, ..., X_n\)</span>，且<span
class="math inline">\(X_i\)</span>的取值范围是<span
class="math inline">\([a_i, b_i]\)</span>，这<span
class="math inline">\(n\)</span>个随机变量的经验期望<span
class="math inline">\(\bar{X}=\frac{X_1 + \dots +
X_n}{n}\)</span>满足以下不等式：</p>
<p><span class="math display">\[P(|\bar{X} - E(\bar{X})| \ge \epsilon)
\ge \exp(-\frac{2n^2\epsilon^2}{\sum_{i=1}^n(b_i - a_i)^2})\]</span></p>
<p><span class="math display">\[P(|\bar{X} - E(\bar{X})| \ge \epsilon)
\le 2 \exp(-\frac{2n^2\epsilon^2}{\sum_{i=1}^n(b_i -
a_i)^2})\]</span></p>
<p>其中<span class="math inline">\(E(\bar{X})\)</span>是<span
class="math inline">\(\bar{X}\)</span>的期望，<span
class="math inline">\(\epsilon\)</span>是一个正数。</p>
<p>VFDT系统解决了Hoeffding
tree算法没有提到的实际问题，就是当两个属性的信息熵差不多时，这个时候就会发生两个属性之间的权衡。这是系统需要花费大量的时间和空间，利用更多的样本来决定选择哪个属性为最佳的决策节点的属性，而这显然是浪费的。</p>
<p>VFDT算法相较于Hoeffding Tress算法的改进：</p>
<ul>
<li>提供了一个用户定义的阈值<span class="math inline">\(τ\)</span>
用来解决“两个属性的信息熵差不多时的博弈”。当信息熵差值小于某个阈值时，即可判定其为决策节点属性。</li>
<li>允许设定节点的最小样本个数值<span
class="math inline">\(n_{min}\)</span>，在用户能够承受的置信度下，让用户设定每个节点最小的样本数将有效的减少样本信息熵<span
class="math inline">\(G\)</span>的计算而消耗的时间复杂度。</li>
<li>提供重新扫描数据集和二次抽样的功能，并且在数据流中的样本数减少时，决策树的精度也会无限逼近于读取所有样本建立决策树的精度。</li>
</ul>
<p><strong>以下是 VFDT 算法的基本流程</strong>：</p>
<ol type="1">
<li><p>构建决策树：对于一个分类问题，首先需要构建一颗决策树。该决策树会被
VFDT 算法不断地更新和重新构建。</p></li>
<li><p>建立示例集：随机选择一些实例作为示例集。</p></li>
<li><p>计算初始统计信息：对于示例集中的每个实例，计算它们分类结果的概率分布。</p></li>
<li><p>增量统计每个实例：对于新增加的每个实例，将其加入当前的示例集，并更新分类结果的概率分布。</p></li>
<li><p>检查增量误差：计算每个分类器的误差，并选择一个误差最小的分类器来更新决策树。</p></li>
<li><p>执行更新：将当前分类器放到决策树上对应的位置，并更新决策树。</p></li>
</ol>
<p>重复步骤 4-6，直到决策树满足一定条件。</p>
<p>VFDT
算法是一种增量式建树算法，它通过不断更新决策树的方法来尽可能地减小误差。这种算法的好处是，可以随时加入新的数据，更新模型，同时不需要重新训练整个模型。但是，VFDT
算法也存在一些缺点，比如计算复杂度较高，对异常数据较为敏感等。</p>
<h5 id="vfdt的优缺点">VFDT的优缺点</h5>
<p><strong>优点</strong>：</p>
<ul>
<li>Scales better than traditional methods(比传统方法更好)
<ul>
<li>Sublinear with sampling(子线性采样)</li>
<li>Very small memory utilization(非常小的内存使用率)</li>
</ul></li>
<li>Incremental(增量学习)
<ul>
<li>Make class predictions in parallel(并行预测分类)</li>
<li>New examples are added as they come(新的样本随着到来而添加)</li>
</ul></li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>Could spend a lot of time with ties(可能会花费很多时间)</li>
<li>Memory used with tree expansion(内存使用率随着树的扩展而增加)</li>
<li>Number of candidate attributes(候选属性的数量大)</li>
</ul>
<h4
id="cvfdtconcept-adapting-very-fast-decision-tree">CVFDT(Concept-adapting
very fast decision tree)</h4>
<p>CVFDT是VFDT的改进版，它保持了VFDT的精度和速度，VFDT
算法假设所分析处理的数据流是平稳分布的，所以应对数据流中概念变化时采用的是单一的决策树模型，这就导致VFDT的决策树模型不能及时反映数据流随时间变化的趋势。</p>
<p>另外VFDT也没有处理连续值属性的问题。因为在CVFDT
中滑动窗口的引入，过时的样本都被删除，所以 CVFDT 树比 VFDT 树要小很多。
CVFDT根据滑动窗口中的数据流样本来持续检测旧的决策树的有效性从而保证建立模型与概念漂移同步。</p>
<p>CVFDT算法对VFDT算法的改进如下：</p>
<ul>
<li><strong>CVFDT算法解决了VFDT算法不能处理数据流中概念漂移的问题</strong>。通过在VFDT算法基础上添加滑动窗口使得建立决策树模型的数据流能够不断实现更新，保证在概念漂移的数据流中保持模型的准确率。</li>
<li>对于每个节点包括根节点都有相应的ID。样本遍历每个节点时不仅会在节点处保存其样本的属性信息，同时窗口中的样本也会保存其遍历过的节点信息。当样本滑出窗口时，该样本所经历过的节点统计值将依次减一。</li>
<li>CVFDT还为每个决策节点设置备选子树，周期性的检测每个决策节点的准确率从而决定替代子树是否替换当前的决策节点，从而也有效的提高了决策树模型的准确率。</li>
</ul>
<p>CVFDT算法流程如下：</p>
<p><img src="/img/大数据/ch5-CVFDT-流程.png" srcset="/img/loading.gif" lazyload /></p>
<p>CVFDT算法采用增量的方式训练决策树，解决了VFDT算法不能处理连续属性的问题，并且在处理大规模数据时效率更高。</p>
<h4 id="syncstream">SyncStream</h4>
<p>KNN style</p>
<!-- 待完善 -->
<h4 id="open-set-problem">Open-set problem</h4>
<ul>
<li>Novel class Dection(Extreme Value Theory,EVT)</li>
</ul>
<p>EVT 中心思想是概率分布，可给出事件发生概率的数学公式。</p>
<ul>
<li>Continued learning(Elastic Weight Consolidation,EWC)</li>
</ul>
<p>EWC的基本思想：模型中的一些参数对前面的任务很重要。只改变不重要的参数</p>
<p>Gradient Episodic Memory
(GEM)的基本思想:限制梯度的方向来改善之前的工作</p>
<ul>
<li>Class-incremental learning</li>
</ul>
<p>问题：</p>
<ol type="1">
<li>怎么平衡新旧类的样本</li>
<li>怎么平衡新旧类的样本的重要性</li>
<li>怎么提取榜样样本(exemplars examples)</li>
</ol>
<p><strong>Knowledge Distillation(知识蒸馏)</strong>:</p>
<p><strong>Weight Aligning(权重对齐)</strong>:
通过对齐权重来减少模型的参数数量</p>
<h3 id="data-stream-clustering流聚类">Data Stream
Clustering(流聚类)</h3>
<h4 id="framework">Framework</h4>
<p>有两个阶段</p>
<ol type="1">
<li>online Data
abstraction(数据抽象):将数据归纳为具有内存效率的数据结构</li>
<li>offline clustering(离线聚类):使用聚类算法来寻找数据类别</li>
</ol>
<!-- online阶段首先根据K-mens算法生成p个初始的聚类中心(micro clusters)，并为每一个簇提供一个独一无二的ID，其中P是大于具体的聚类数目但是要远远小于具体数据点的个数。

对于每一个到来的数据点，要么被现有的微集群吸收（是否在一个集群的最大边界内-均值根偏差RMS），要么自己成立一个集群。但是数据点不属于现有的集群有两种情况，一种是该点是一个异常点，第二种是该点是一个新集群的起始点。

那么如果要新建立一个集群的话，就需要将已有的集群删除一个或者合并两个相似的集群。若要删除一个集群的话，首先判断删除该集群是否会有不良的影响，所以根据该集群的数据点的时间戳信息来判断，如果该簇的时间戳不满足设定的阈值，将其删除。合并集群的话，将两个最近的集群进行合并。 -->
<p>流聚类算法：</p>
<p><img src="/img/大数据/ch5-流聚类算法.png" srcset="/img/loading.gif" lazyload /></p>
<!-- offline clustering(离线聚类)

根据用户输入的需要查看的时间，从特征金字塔中取出两个时间段的汇总信息，相减之后就得到用户所需时间段的近似数据集，在此基础上进行聚类即可。 -->
<h4 id="online-data-abstraction">online Data abstraction</h4>
<p><strong>Micro-Cluster</strong>: A Micro-Cluster is a set of
individual data points that are close to each other and will be treated
as a single unit in further offline
Macro-clustering.(Micro-Cluster是一组彼此接近的单个数据点，将在进一步的离线宏聚类中作为单个单元处理。)</p>
<p><strong>Cluster Feature</strong>:
用来表示一个Micro-Cluster的属性，<span class="math inline">\(CF = (N,
LS, SS)\)</span></p>
<p>其中<span class="math inline">\(LS = \sum\limits_{i=1}^N X_i, SS =
\sum\limits_{i=1}^N X_i^2\)</span></p>
<p>其中<span class="math inline">\(N\)</span>是数据点，<span
class="math inline">\(LS,SS\)</span>中的<span
class="math inline">\(X_i\)</span>是一个向量。</p>
<p><strong>Cluster Feature</strong>的属性：Additivity Property</p>
<p><img src="/img/大数据/ch5-CF属性-1.png" srcset="/img/loading.gif" lazyload /> <img
src="/img/大数据/ch5-CF属性-2.png" srcset="/img/loading.gif" lazyload /></p>
<ol type="1">
<li>动态选择short-term和long-term的代表性example，代表性高的保留，代表性低的删除，没有，若代表性没有改变，就进行statistic
summary(摘要统计)</li>
<li>Cluster Feature的属性：Additivity Property</li>
</ol>
<h2 id="ch6-graph-mining">Ch6 Graph Mining</h2>
<h3 id="key-node-identification">Key Node Identification</h3>
<h4 id="centrality">Centrality</h4>
<p><strong>Degree
Centrality</strong>:节点度的大小用来衡量节点的重要性（节点的直接影响）。</p>
<p><strong>Betweenness
Centrality</strong>：每个顶点的间性中心度是通过该顶点的这些最短路径的数量。</p>
<p><strong>Closeness
Centrality</strong>：计算为节点与图中所有其他节点之间的最短路径长度之和。</p>
<h3 id="k-shell-decomposition">K-shell Decomposition</h3>
<p>将图中结点度为1的所有结点和对应的连边去掉后，新的网络中可能会有新的度为1的结点，把这些结点和边也去掉，重复操作，直到不再有度为1的结点为止。这种操作类似于剥去网络最外面一层壳，所以把所有去除的结点以及他们之间的连边称为网络的1-壳(1-shell)。网络中度为0的独立结点称为0-壳(0-shell)。在去除1-壳后的网络中，所有结点度都大于等于2，因此，接着把度为2的结点和对应连边去掉，直到不再有度为2的结点为止，则去除的结点和边称为2-壳(2-shell)。依此类推，直到网络中每个结点都划分到相应k-shell中，就得到网络的k-shell分解。</p>
<p>每个结点都唯一对应一个k-shell，这个k-shell中的结点的度一定大于等于k。但是注意，度相同的结点不一定属于同一个k-shell。并且，度大的结点既可能属于k值大的k-shell(最内层)，可能能属于k值较小的shell(外层)。所以，度值大的未必就重要。</p>
<p>优点：</p>
<ul>
<li>计算复杂度低</li>
<li>直观的揭示了网络的层次结构</li>
</ul>
<p>缺点：</p>
<ul>
<li>不能在很多网络中使用，如星形网络、树形网络等。</li>
<li>不能很好地反映网络的重要性，有时候甚至不如单纯用节点度值衡量效果好。</li>
</ul>
<h3
id="eigenvector特征向量e.g.pagerank">Eigenvector（特征向量）e.g.PageRank</h3>
<p>PageRank是Google最早的搜索引擎核心用的就是这个算法</p>
<p>PageRank基本思想：如果一个页面被很多其他页面链接到的话说明这个页面比较重要，如果一个页面被一个很重要的页面链接到的话，那么这个页面也很重要。</p>
<h3 id="community-detection">Community Detection</h3>
<h4 id="cut-based-methods">Cut-based Methods</h4>
<h5 id="minimum-cut">Minimum Cut</h5>
<p>由于大多数节点互动是在组内进行的，而组与组之间的互动则很少。所以我们可以把community
detection问题转换为一个最小割问题。最小割问题是指在一个无向图中，找到一条边的集合，使得这些边的权重之和最小，且删除这些边之后，图被分成两个部分。</p>
<h5 id="ratio-cut-normalized-cut">Ratio Cut &amp; Normalized Cut</h5>
<p>由于最小割往往返回一个不平衡的partation，其中的一个集合是一个单点，所以我们可以用Ratio
Cut和Normalized
Cut来解决这个问题。思想是在最小割的基础之上修改目标函数以考虑到partation的平衡性。</p>
<p><img src="/img/大数据/ch6-ratio-cut.png" srcset="/img/loading.gif" lazyload /></p>
<p><img src="/img/大数据/ch6-norm-cut.png" srcset="/img/loading.gif" lazyload /></p>
<p>其中<span class="math inline">\(C_i\)</span>是一个子集和，<span
class="math inline">\(\bar{C_i}\)</span>是<span
class="math inline">\(C_i\)</span>的补集，<span
class="math inline">\(|C_i|\)</span>是子集和的大小，<span
class="math inline">\(vol(C_i)\)</span>是子集和中所有节点的度之和。</p>
<p>在 RatioCut
切图中，不仅要考虑使不同组之间的权重最小化，也考虑了使每个组中的样本点尽量多。</p>
<p>在 Norm Cut
切图中，除了考虑最小化损失函数之外，还考虑了子图之间的权重大小。</p>
<p>由于子图样本的个数多并不一定权重就大，切图时基于权重也更合目标，因此一般来说Norm
cut 切图优于 RatioCut 切图。</p>
<h5 id="modularity-maximization模块度最大化">Modularity
Maximization(模块度最大化)</h5>
<p>模块度通过考虑度分布来衡量Community partition的强度。</p>
<p>给定一个有<span
class="math inline">\(m\)</span>条边的网络，学位为<span
class="math inline">\(d_i\)</span>和<span
class="math inline">\(d_j\)</span>的两个节点之间的预期边数为<span
class="math inline">\(d_i d_j / 2 m\)</span></p>
<p>给定如下例子：</p>
<p><img src="/img/大数据/ch6-Modularity-Maximization-例子图.png" srcset="/img/loading.gif" lazyload /></p>
<p>则节点<span class="math inline">\(1\)</span>和<span
class="math inline">\(2\)</span>之间的预期边数为<span
class="math inline">\(3 * 2 / (2 * 14)\)</span></p>
<p>Strength of a community: <span class="math inline">\(\sum\limits_{i
\in C, j \in C} A_{ij} - \frac{d_i d_j}{2m}\)</span></p>
<p>Modularity(模块度)：<span class="math inline">\(Q =
\frac{1}{2m}\sum\limits_{l =1}^k \sum\limits_{i \in C, j \in C} (A_{ij}
- \frac{d_i d_j}{2m})\)</span> )</p>
<p>数值越大，表明community structure越好.</p>
<h5 id="simulatingdistance-dynamics">Simulating:Distance Dynamics</h5>
<p>动态距离(Distance Dynamics)是Community
Detection的一种新视角，它的基本思想是模拟边距离的动态变化。</p>
<p>它将整个网络视为一个动态系统，根据不同的互动模式模拟距离动态（距离动态与节点动态）.所有边的距离都会收敛，从而直观地识别出社区结构。</p>
<p>如果两个节点相连，每个节点都会吸引另一个节点，使得另一个节点移动到自己身边.</p>
<p>边距离:受三种不同类型节点的影响:(a)直接链接节点;(b)共同邻居;(c)独占邻居</p>
<ul>
<li>直接链接节点的影响：使u和v更接近。</li>
<li>共同邻居的影响：使u和v更接近。</li>
<li>独占邻居的影响：使u和v更接近或更远。</li>
</ul>
<p>Distance Dynamics的步骤：</p>
<ol type="1">
<li>Initialization：计算每条边的Jaccard距离</li>
<li>Dynamics: 研究每条边距离的变化</li>
<li>Community Detection : 删除距离为1的边</li>
</ol>
<p><img src="/img/大数据/ch6-Distance-Dynamics-流程.png" srcset="/img/loading.gif" lazyload /></p>
<h3 id="graph-embedding">Graph embedding</h3>
<h4 id="motivation">Motivation</h4>
<ul>
<li>网络包含数十亿的节点和边，对整个网络进行复杂的推理是难以实现的。</li>
<li>机器学习算法需要向量表示</li>
<li>如何在Graph embedding的过程中保留Commmunity structure</li>
<li>如何有效地处理大规模网络</li>
</ul>
<p>Graph embedding的目标是将每个节点映射到一个低维空间。</p>
<p>图是一种非欧几里得结构，图的属性：</p>
<ul>
<li>节点的编号是任意的</li>
<li>图具有任意的大小</li>
<li>结构复杂</li>
</ul>
<p>困难：</p>
<ul>
<li>衡量节点之间的相似度</li>
<li>编码网络信息并生成节点表示</li>
</ul>
<h4 id="deepwalk">DeepWalk</h4>
<!-- DeepWalk的思想类似word2vec，使用图中节点与节点的共现关系来学习节点的向量表示。那么关键的问题就是如何来描述节点与节点的共现关系，DeepWalk给出的方法是使用随机游走(RandomWalk)的方式在图中进行节点采样。

RandomWalk是一种可重复访问已访问节点的深度优先遍历算法。给定当前访问起始节点，从其邻居中随机采样节点作为下一个访问节点，重复此过程，直到访问序列长度满足预设条件。

![](/img/大数据/ch6-deepwalk-流程.png)

#### Node2Vec

**优化目标**:

设$f(u)$是是将顶点 $u$ 映射为embedding向量的映射函数,对于图中每个顶点$u$,定义$N_{S}(u)$为通过采样
策略$S$采样出的顶点$u$的近邻顶点集合。

node2vec优化的目标是给定每个顶点条件下，令其近邻顶点（**如何定义近邻顶点很重要**）出现的概率最大。

![](/img/大数据/ch6-Node2vec-公式.png) -->
<p>详见<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/56380812">【Graph
Embedding】DeepWalk：算法原理，实现和应用</a></p>
<h4 id="node2vec">Node2vec</h4>
<p>详见<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/56542707">【Graph
Embedding】node2vec：算法原理，实现和应用</a></p>
<h2 id="ch7-hadoopspark">Ch7 Hadoop/Spark</h2>
<h3 id="hadoop">Hadoop</h3>
<h4 id="what-is-hadoop">What is Hadoop</h4>
<p>Hadoop是一个软件框架，用于在大型计算机集群中分布式处理大型数据集。</p>
<h4 id="design-principles-of-hadoop">Design Principles of Hadoop</h4>
<ul>
<li>Need to process big data</li>
<li>Need to parallelize computation across thousands of nodes</li>
<li>Commodity hardware
<ul>
<li>Large number of low-end cheap machines working in parallel to solve
a computing problem</li>
</ul></li>
<li>This is in contrast to Parallel DBs
<ul>
<li>Small number of high-end expensive machines</li>
</ul></li>
<li>Automatic parallelization &amp; distribution
<ul>
<li>Hidden from the end-user</li>
</ul></li>
<li>Fault tolerance(容错) and automatic recovery
<ul>
<li>Nodes/tasks will fail and will recover automatically</li>
</ul></li>
<li>Clean and simple programming abstraction.(干净而简单的编程抽象)
<ul>
<li>Users only provide two functions “map” and “reduce”</li>
</ul></li>
</ul>
<h4 id="hadoop-architecture">Hadoop Architecture</h4>
<ul>
<li>Distributed file system (HDFS)</li>
<li>Execution engine (MapReduce)</li>
</ul>
<p><img src="/img/大数据/ch7-Hadoop-Architecture.png" srcset="/img/loading.gif" lazyload /></p>
<h3 id="eco-system-of-hadoop">Eco-system of Hadoop</h3>
<ul>
<li>HDFS:Storing(存储)</li>
<li>MapReduce:computation</li>
<li>HBASE:NoSQL database</li>
<li>Hive:Data warehouse(数据仓库)</li>
<li>Pig:Data flow language</li>
<li>Zookeeper:Coordination service</li>
<li>Core:Filesystems and I/O</li>
<li>Avro:Cross-language serialization(跨语言序列化)</li>
</ul>
<h3 id="hadoop-distributed-file-systemhdfs分布式文件系统">Hadoop
Distributed File System(HDFS)：分布式文件系统</h3>
<p><strong>Main Properties of HDFS</strong>:</p>
<ul>
<li><strong>Large</strong>:一个HDFS实例可能由数以千计的服务器机器组成，每个机器都存储着文件系统的部分数据</li>
<li><strong>Replication</strong>:每个数据块被多次复制（默认为3）。</li>
<li><strong>Failure</strong>:失败是常态而不是例外</li>
<li><strong>Fault
Tolerance</strong>:检测故障和快速自动恢复故障是HDFS的一个核心架构目标</li>
</ul>
<h4 id="namenode-datanode"><strong>NameNode + DataNode</strong></h4>
<ul>
<li>NameNode(meta-information)
<ul>
<li>Managing FsImage file and EditLog file to manager meta
information</li>
<li>EditLog is used to update FsImage (Checkpoint).</li>
</ul></li>
<li>DataNode(actual data)
<ul>
<li>Store data</li>
<li>Block operation</li>
</ul></li>
</ul>
<h4 id="fault-tolerance-replication-heartbeat"><strong>Fault
Tolerance</strong>: Replication + HeartBeat</h4>
<ul>
<li>HeartBeat:DataNode</li>
<li>Replication:Steady NameNode</li>
</ul>
<h3 id="mapreduce">MapReduce</h3>
<p>MapReduce的思想就是“分而治之”</p>
<h4 id="map">Map</h4>
<p>Mapper负责“分”，即把复杂的任务分解为若干个“简单的任务”来处理。“简单的任务”包含三层含义：</p>
<ul>
<li>一是数据或计算的规模相对原任务要大大缩小;</li>
<li>二是就近计算原则，即任务会分配到存放着所需数据的节点上进行计算;</li>
<li>三是这些小任务可以并行计算，彼此间几乎没有依赖关系。</li>
</ul>
<h4 id="reduce">Reduce</h4>
<p>Reducer负责对map阶段的结果进行汇总。至于需要多少个Reducer，用户可以根据具体问题具体设置。</p>
<h3 id="spark基于内存的计算框架">Spark(基于内存的计算框架)</h3>
<p>MapReduce is great at one-pass computation,but inefficient for
multi-pass algorithms.</p>
<h4 id="what-is-spark">What is Spark</h4>
<p><strong>Apache Spark is a fast and general-purpose cluster computing
system.</strong>It also supports a rich set of higher-level tools
including <strong>Spark SQL</strong> for SQL and structured data
processing,<strong> MLlib</strong> for machine
learning,<strong> GraphX</strong> for graph processing,
and <strong>Spark Streaming</strong> for streaming processing.</p>
<h4 id="memory-based-computation">Memory based computation</h4>
<h5 id="rdd">RDD</h5>
<p>Spark的主要抽象是resilient distributed dataset
(RDD),它表示一个只读的对象集合，在一组机器上进行分区，如果一个分区丢失，可以重建。</p>
<p>An RDD can be created 2 ways:</p>
<ul>
<li>Parallelize a collection</li>
<li>Read data from an external source</li>
</ul>
<h5 id="operations-on-rdd">Operations on RDD</h5>
<ul>
<li>transformations：create a new dataset from an existing one</li>
<li>actions: return a value to the driver program after running a
computation on the dataset</li>
</ul>
<h4 id="fault-tolerance">Fault Tolerance</h4>
<p>暂无待续.</p>
<h3 id="mapreduce-vs-spark">MapReduce VS Spark</h3>
<p><strong>MapReduce</strong>:</p>
<ul>
<li>Great at <strong>one-pass computation</strong>, but inefficient for
<strong>multi-pass algorithms</strong>.</li>
<li>No efficient primitives for data
sharing(没有用于数据共享的有效基元)</li>
</ul>
<p><strong>Spark</strong>:</p>
<ul>
<li>Extends a programming language with a distributed collection
data-structure（RDD）.(用分布式集合数据结构（RDD）扩展了一种编程语言)</li>
<li>Clean APIs in Java, Scala, Python, R.</li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>大数据期末复习</div>
      <div>https://gstarmin.github.io/2023/06/02/大数据期末复习/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Starmin</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年6月2日</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>更新于</div>
          <div>2023年6月7日</div>
        </div>
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/06/05/WSL%E4%B8%8B%E6%B7%BB%E5%8A%A0%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8/" title="WSL下添加系统调用">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">WSL下添加系统调用</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/06/01/%E5%89%8D%E7%BC%80%E5%92%8C%E6%95%B0%E7%BB%84%E4%B8%8E%E5%B7%AE%E5%88%86%E6%95%B0%E7%BB%84/" title="前缀和数组与差分数组">
                        <span class="hidden-mobile">前缀和数组与差分数组</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
