

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Starmin">
  <meta name="keywords" content="">
  
    <meta name="description" content="大数据期末复习 Ch1 Intruction What is big data? Big data is used to describe a massive volume of both structured and unstructured data that is so large that it&#39;s difficult to process using traditional databa">
<meta property="og:type" content="article">
<meta property="og:title" content="大数据期末复习">
<meta property="og:url" content="https://gstarmin.github.io/2023/06/02/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/">
<meta property="og:site_name" content="私人杂货铺">
<meta property="og:description" content="大数据期末复习 Ch1 Intruction What is big data? Big data is used to describe a massive volume of both structured and unstructured data that is so large that it&#39;s difficult to process using traditional databa">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/4V-features.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/KDD-process.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch2-lossfunction.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch3-the-big-pic.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch3-similarity.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch3-signature计算.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/Partion-Into-Bands.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/Partion-Into-Bands-例子.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/Spectral-Hashing公式.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch3-Learning-Based-Hashing-步骤.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch4-CDF.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch4-Rejection-Sampling.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch4-Rejection-Sampling-步骤.png">
<meta property="og:image" content="https://gstarmin.github.io/2023/06/02/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/img/大数据/ch4-MCMC流程.png">
<meta property="og:image" content="https://gstarmin.github.io/2023/06/02/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/img/大数据/ch4-MH算法流程.png">
<meta property="og:image" content="https://gstarmin.github.io/2023/06/02/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/img/大数据/ch4-MH算法具体流程.png">
<meta property="og:image" content="https://gstarmin.github.io/2023/06/02/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/img/大数据/ch4-Gibbs.png">
<meta property="og:image" content="https://gstarmin.github.io/2023/06/02/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/img/大数据/ch4-Gibbs-二维流程.png">
<meta property="og:image" content="https://gstarmin.github.io/2023/06/02/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/img/大数据/ch4-Gibbs-多维流程.png">
<meta property="og:image" content="https://gstarmin.github.io/2023/06/02/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/img/大数据/ch4-Reservoir.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch4-CVFDT-流程.png">
<meta property="og:image" content="https://gstarmin.github.io/img/大数据/ch5-流聚类算法.png">
<meta property="article:published_time" content="2023-06-02T09:38:02.000Z">
<meta property="article:modified_time" content="2023-06-04T14:30:00.000Z">
<meta property="article:author" content="Starmin">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://gstarmin.github.io/img/大数据/4V-features.png">
  
  
  
  <title>大数据期末复习 - 私人杂货铺</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"gstarmin.github.io","root":"/","version":"1.9.3","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 5.4.2"><style>.mjpage .MJX-monospace {
  font-family: monospace;
}

.mjpage .MJX-sans-serif {
  font-family: sans-serif;
}

.mjpage {
  display: inline;
  font-style: normal;
  font-weight: normal;
  line-height: normal;
  font-size: 100%;
  font-size-adjust: none;
  text-indent: 0;
  text-align: left;
  text-transform: none;
  letter-spacing: normal;
  word-spacing: normal;
  word-wrap: normal;
  white-space: nowrap;
  float: none;
  direction: ltr;
  max-width: none;
  max-height: none;
  min-width: 0;
  min-height: 0;
  border: 0;
  padding: 0;
  margin: 0;
}

.mjpage * {
  transition: none;
  -webkit-transition: none;
  -moz-transition: none;
  -ms-transition: none;
  -o-transition: none;
}

.mjx-svg-href {
  fill: blue;
  stroke: blue;
}

.MathJax_SVG_LineBox {
  display: table !important;
}

.MathJax_SVG_LineBox span {
  display: table-cell !important;
  width: 10000em !important;
  min-width: 0;
  max-width: none;
  padding: 0;
  border: 0;
  margin: 0;
}

.mjpage__block {
  text-align: center;
  margin: 1em 0em;
  position: relative;
  display: block !important;
  text-indent: 0;
  max-width: none;
  max-height: none;
  min-width: 0;
  min-height: 0;
  width: 100%;
}
</style></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Starmin</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="大数据期末复习"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-06-02 17:38" pubdate>
          2023年6月2日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          13k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          132 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">大数据期末复习</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="大数据期末复习">大数据期末复习</h1>
<h2 id="ch1-intruction">Ch1 Intruction</h2>
<h3 id="what-is-big-data">What is big data?</h3>
<p><strong>Big data</strong> is used to describe a massive volume of
both structured and unstructured data that is so large that it's
difficult to process using traditional database and software
techniques.</p>
<p><strong>大数据</strong>用来描述大量的结构化和非结构化的数据，这些数据非常大，难以用传统的数据库和软件技术来处理。</p>
<h3 id="the-4v-features-of-big-data">The 4V Features of big data</h3>
<p><img src="/img/大数据/4V-features.png" srcset="/img/loading.gif" lazyload /></p>
<ul>
<li>Volume (Scale of Data)</li>
<li>Velocity (Data Stream)</li>
<li>Variety (Different types of data)</li>
<li>Veracity (Uncertainty, missing value)</li>
</ul>
<h3 id="what-is-data-mining">What is data mining?</h3>
<p><strong>Data mining</strong> consists of applying data analysis and
discovery algorithms that, under acceptable computational efficiency
limitations, produce a particular enumeration of patterns over the
data.</p>
<p><strong>数据挖掘</strong>包括应用数据分析和发现算法，在可接受的计算效率限制下，在数据上产生特定的模式列举。</p>
<h3 id="the-kdd-processcore-part">The KDD Process(core part)</h3>
<p><img src="/img/大数据/KDD-process.png" srcset="/img/loading.gif" lazyload /></p>
<h3 id="the-main-tasks-of-data-mining">The main tasks of Data
mining</h3>
<ul>
<li>Association Rule Mining(关联规则挖掘)</li>
<li>Cluster Analysis(聚类分析)</li>
<li>Classification/Prediction(分类/预测)</li>
<li>Outlier Detection(异常点检测)</li>
</ul>
<h3
id="the-relationship-between-data-minning-and-other-subjectse.g.-database">The
relationship between Data minning and other subjects(e.g. Database)</h3>
<p>Data mining is known as Knowledge Discovery in Database (KDD) in the
field of artificial intelligence, is also considered as a fundamental
step in the process of knowledge discovery in database.</p>
<p>数据挖掘在人工智能领域被称为数据库知识发现（KDD），也被认为是数据库知识发现过程中的一个基本步骤。</p>
<h3 id="the-challenges-of-big-data-mining">The challenges of big data
mining</h3>
<ul>
<li>Curse of dimensionality</li>
<li>Storage cost</li>
<li>Query speed <!-- - Data Quality
- Data Complexity
- Data Privacy and Security
- Scalability
- Ethics(伦理学)
- interpretability --></li>
</ul>
<h2 id="ch2-foundations-of-data-mining">Ch2 Foundations of Data
Mining</h2>
<h3
id="supervised-learningunsupervised-learningsemi-supervised-learning">Supervised
learning/Unsupervised learning/Semi-supervised learning</h3>
<ul>
<li><strong>Supervised learning</strong>: targets to learn the mapping
function or relationship between the features and the labels based on
the labeled data. Namely, <span class="math inline">\(𝑌=𝐹(𝑋|𝜃)\)</span>.
(e.g. Classification, Prediction)</li>
<li><strong>Unsupervised learning</strong>: aims at learning the
intrinsic structure from unlabeled data. (e.g. Clustering, Latent Factor
Learning and Frequent Items Mining)</li>
<li><strong>Semi-supervised learning</strong>: can be regarded as the
unsupervised learning with some constraints on labels, or the supervised
learning with additional information on the distribution of data.</li>
</ul>
<h3 id="loss-function">LOSS FUNCTION</h3>
<p><img src="/img/大数据/ch2-lossfunction.png" srcset="/img/loading.gif" lazyload /></p>
<p><span class="math inline">\(l_1\)</span> norm:</p>
<p><span class="math display">\[L(\beta) =\frac{1}{N}\sum\limits_{i =
1}^{N}L(Y_i, F(X_i | \beta)) + \frac{\lambda}{2} || \beta
||_2\]</span></p>
<p><span class="math inline">\(l_2\)</span> norm:</p>
<p><span class="math display">\[L(\beta) =\frac{1}{N}\sum\limits_{i =
1}^{N}L(Y_i, F(X_i | \beta)) + \frac{\lambda}{2} || \beta
||_1\]</span></p>
<p><span class="math inline">\(||A||_∗\)</span> nuclear norm:</p>
<p><span class="math display">\[||A||∗=∑_i{σ_i}(A).\]</span></p>
<h3 id="overfittingunderfitting-problem">Overfitting/Underfitting
problem</h3>
<p><strong>Reason</strong>?</p>
<p><strong>How to avoid overfitting</strong>?</p>
<h3 id="classfied-algorithms">Classfied Algorithms</h3>
<h4 id="decision-tree">Decision Tree</h4>
<ul>
<li>How to construct DT?</li>
<li>Attribute selection Criteria
<ul>
<li>Information Gain</li>
<li>Information Gain Ratio</li>
<li>Gini index</li>
</ul></li>
</ul>
<h4 id="knn">KNN</h4>
<p><strong>Lazy Learning</strong>: Lazy Learning does not extract rules
or generalizations from a specific model. Instead, it searches for
historical instances that are similar to the testing instance and makes
a prediction based on their output results.(Lazy
Learning并没有从特定的模型中提取基本规则或一般情况，而是在预测时查找与测试实例相似的历史实例，并根据它们的输出结果做出预测)</p>
<p><strong>advantage</strong>:</p>
<ul>
<li>local data distribution（适用本地数据分布）</li>
<li>Incremental/online learning（渐进式/在线学习）</li>
<li>large number of classes（可以对很大的类型数量分类）</li>
</ul>
<p><strong>disvantage</strong>:</p>
<ul>
<li>parameter k(要设置参数k)</li>
<li>imbalanced data（数据不平衡时分类效果差）</li>
<li>slow inference（推理慢）</li>
</ul>
<h4 id="naive-bayse">Naive bayse</h4>
<p><strong>basic idea</strong></p>
<p><strong>advantage</strong></p>
<h4 id="svm">SVM</h4>
<ol type="1">
<li>basic concept</li>
<li>Linear seperation problem
<ol type="1">
<li>Why SVM works well on small size of samples?</li>
<li>Good generalization</li>
</ol></li>
<li>NonLinear problem
<ol type="1">
<li>solution: map data into high dimension space</li>
<li>Trick: kernel Trick <span class="math inline">\(K(X,Y) = \Phi(X)
\Phi(Y)\)</span></li>
<li>Kernel function: Gaussian kernel, polynormial kernel</li>
</ol></li>
</ol>
<h4 id="section"></h4>
<p>损失函数是平方损失加上L1正则化</p>
<p><span class="math display">\[J(\theta) = \frac{1}{2m} \sum_{i=1}^m
(h_\theta(x^{(i)}) - y^{(i)})^2 + \alpha \sum_{j=1}^n
|\theta_j|\]</span></p>
<p>其中，第一项是平方损失，第二项是L1正则化项，<span
class="math inline">\(\alpha\)</span>是正则化参数。</p>
<h4 id="subspace-clustering子空间聚类">Subspace
Clustering(子空间聚类)</h4>
<h2 id="ch3-hashing">Ch3 Hashing</h2>
<h3 id="the-role-of-hashing作用">The role of Hashing(作用)</h3>
<ul>
<li>After using the hash code to represent the data, the required
storage space will be greatly
reduced（使用哈希码表示数据后，所需要的存储空间会被大幅减小）</li>
<li>Can reduce data dimensionality, thereby alleviating the
dimensionality curse
problem(可以降低数据维度，从而减轻维度灾难问题)</li>
<li>Can realize fast neighbor retrieval at constant or sub-linear level,
and provide support for the rapid realization of upper-level learning
tasks(可以实现常数或者次线性级别的快速近邻检索，为上层学习任务的快速实现提供支撑)</li>
</ul>
<h3 id="find-similar-items">Find similar items</h3>
<p>Three Essential Techniques for Similar items:</p>
<ul>
<li><strong>K-Shingling</strong>：convert documents, emails, etc., to
sets.</li>
<li><strong>Minhashing</strong>：convert large sets to short signatures,
while preserving similarity.</li>
<li><strong>Locality-sensitive hashing</strong> : focus on pairs of
signatures likely to be similar.</li>
</ul>
<p><img src="/img/大数据/ch3-the-big-pic.png" srcset="/img/loading.gif" lazyload /></p>
<h4 id="shingles">Shingles</h4>
<p>A k -shingle (or k -gram) for a document is a sequence of k
characters that appears in the document.
一个文件的k-shingle（或k-gram）是一个出现在文件中的k个字符的序列。</p>
<p>Example: <span class="math inline">\(k=2; doc = abcab\)</span>. Set
of 2-shingles = <span class="math inline">\(\{ab, bc, ca\}\)</span>.</p>
<h4 id="min-hashing">min-hashing</h4>
<p>definition:min-hash is an algorithm for text and data similarity
comparison that efficiently extracts the signature of each data from
large-scale data, thus supporting fast comparison of their similarity.
min-hash是一种用于文本和数据相似度比较的算法，它可以高效地从大规模数据中提取每个数据的签名，从而支持快速地比较它们之间的相似程度。</p>
<h4 id="signature-matrix-rightarrow-how-to-compute-similarity">signature
matrix <span class="math inline">\(\Rightarrow\)</span> how to compute
similarity</h4>
<p><img src="/img/大数据/ch3-similarity.png" srcset="/img/loading.gif" lazyload /></p>
<p>Jaccard similarity：不将<span
class="math inline">\((0,0)\)</span>计入分母，相同的行占全部行的比率</p>
<p>matrix similarity：相同的行占全部行的比率</p>
<p><strong>Signature Matrix的计算方法</strong>：</p>
<p><img src="/img/大数据/ch3-signature计算.png" srcset="/img/loading.gif" lazyload /></p>
<p>上图中间矩阵是输入矩阵，左侧的每一列都代表输入矩阵行的一种排列，那么signature
matrix的每一行都对应左侧的一种排列方式，该行的每列数字对应该种排列方式对应列的第一个1的出现行数。</p>
<h4 id="locality-sensitive-hashinglsh">Locality-Sensitive
Hashing（LSH）</h4>
<p>假设我们在主内存中有代表大量对象的数据</p>
<ul>
<li>可能是对象本身</li>
<li>可能是min-hashing中的签名</li>
</ul>
<p>我们要逐一进行比较，找到那些足够相似的pair。但是检查所有的pair是很困难的。</p>
<ul>
<li>一般的想法：
使用一个函数f(x,y)，告诉人们x和y是否是一个候选对：一对元素的相似性必须被评估。</li>
<li>对于min-hash矩阵：
哈希列到许多桶中，并使同一桶中的元素成为候选对。</li>
</ul>
<p>基本思想：Generate from the collection of all elements (signatures in
our example) a small list of candidate pairs: pairs of elements whose
similarity must be evaluated.</p>
<p>简单来说就是从我们Min-hashing得到的标记矩阵生成可能相似的文档对列表。</p>
<p>候选相似文档对 <span class="math inline">\(\Rightarrow\)</span>
这一对的Jaccard相似度必须被准确计算出来</p>
<p>方法：</p>
<ul>
<li>选一个相似度标准 <span class="math inline">\(t\)</span>，并且 <span
class="math inline">\(t&lt;1\)</span>，如果两个文档的相似度大于 <span
class="math inline">\(t\)</span>，则认为这两个文档相似。</li>
<li>如果列<span class="math inline">\(c\)</span>和列<span
class="math inline">\(d\)</span>被视为候选文档对，那么他们一定要满足
<span
class="math inline">\(M(i,c)=M(i,d)&gt;=t\)</span>，其中M是标记矩阵。</li>
</ul>
<h5 id="lsh-for-minhashing-signatures">LSH for Minhashing
Signatures</h5>
<p>总体思想：把标记矩阵里的hash很多遍，只有hash到同一个桶(bucket)里的列才被认为是可能相似的。</p>
<p><strong>Partion Into Bands</strong></p>
<p><img src="/img/大数据/Partion-Into-Bands.png" srcset="/img/loading.gif" lazyload /></p>
<p>如图所示，把标记矩阵(signature matrix)的所有行分成 <span
class="math inline">\(b\)</span> 个带(bands)，每个带有 <span
class="math inline">\(r\)</span>
行。对于每条带，对带里面每列进行hash，分别hash到<span
class="math inline">\(k\)</span>个桶中，并让<span
class="math inline">\(k\)</span>尽可能得大。</p>
<p>只有有<span
class="math inline">\(&gt;=1\)</span>的band哈希到同一个桶中，就把这两列当作候选相似对。</p>
<p><img src="/img/大数据/Partion-Into-Bands-例子.png" srcset="/img/loading.gif" lazyload /></p>
<h5 id="example---bands">Example - Bands</h5>
<p>假设有 100,000 列，每列有100个标记，因此存储标记需要40MB;
我们希望找到所以相似度大于80%的文档对，用上面的方法，把标记分为20个带，每个带里有5个标记。</p>
<p>这样的话，如果文档<span class="math inline">\(C_1\)</span>和<span
class="math inline">\(C_2\)</span>的相似度是<span
class="math inline">\(80\%\)</span>，那么他们的任意一个带的<span
class="math inline">\(5\)</span>个标记都相同的概率是: <span
class="math inline">\((0.8)^5=0.328\)</span>
，看起来好像不大，但是只要有任意一个带都相同就被认为是候选对，所以他们不被选上的概率，即20个带都不相同的概率为：<span
class="math inline">\((1−0.328)^20=0.00035\)</span> ，也就是每<span
class="math inline">\(3000\)</span>个相似度为<span
class="math inline">\(80\%\)</span>的文档对里才会有一对漏选。</p>
<p>我们再考虑文档<span class="math inline">\(C_1\)</span>和<span
class="math inline">\(C_2\)</span>只有<span
class="math inline">\(40\%\)</span>的相似度，那么他们任意一个带的<span
class="math inline">\(5\)</span>个标记都相同的概率为 <span
class="math inline">\((0.4)^5=0.01\)</span>，则文档<span
class="math inline">\(C_1\)</span>和<span
class="math inline">\(C_2\)</span>被选为候选对的概率，即他们中有一个带完全相同的概率为:
<span class="math inline">\(C^1_{20}×0.01=0.2\)</span> ，就是说每<span
class="math inline">\(5\)</span>个<span
class="math inline">\(40\%\)</span>相似度的文档对里就有一对会被误选为候选对。但是相似度小于<span
class="math inline">\(40\%\)</span>的文档对里误选的概率就非常小了。</p>
<h3 id="learn-to-hash">Learn to Hash</h3>
<ol type="1">
<li>Data indenpendent:Random projection</li>
<li>Data dependent:
<ol type="1">
<li>PCA hashing</li>
<li>Spectral Hashing</li>
</ol></li>
</ol>
<h4 id="pca-hashing">PCA hashing</h4>
<p>分为两个阶段</p>
<p><strong>Projection Stage（投影阶段）</strong>:</p>
<p>用一个转换矩阵<span class="math inline">\(W\)</span>,可以将<span
class="math inline">\(x\)</span>投影到一个新的特征平面。</p>
<p><span class="math display">\[Y=W^T X\]</span></p>
<p><strong>Quantization Stage（量化阶段）</strong>:</p>
<p><span class="math display">\[h(x) = sgn(W^T X)\]</span></p>
<p>最小化quantization loss（量化损失）</p>
<p><span class="math display">\[Q(B,Y) = ||B - Y^T R||^2_F\]</span></p>
<p><span class="math inline">\(R\)</span>是正交矩阵. <span
class="math inline">\(B = Sgn(Y^T R)\)</span></p>
<p>基本思想是旋转数据以最小化量化损失。</p>
<p>实现方法：从<span
class="math inline">\(R\)</span>的随机初始化开始，采用类似K-means的迭代算法来优化<span
class="math inline">\(R\)</span>。在每次迭代中，每个数据点首先被分配到最近的聚类中心，然后更新<span
class="math inline">\(R\)</span>以使量化损失最小化。</p>
<h4 id="spectral-hashing谱哈希">Spectral Hashing（谱哈希）</h4>
<p><img src="/img/大数据/Spectral-Hashing公式.png" srcset="/img/loading.gif" lazyload /></p>
<h4
id="general-approach-to-learning-based-hashinglearning-based哈希的一般方法">General
Approach to Learning-Based Hashing(Learning-Based哈希的一般方法)</h4>
<p>将哈希学习问题分解为两个步骤：</p>
<ol type="1">
<li>哈希比特学习</li>
<li>基于所学习的哈希比特的哈希函数学习</li>
</ol>
<p><img src="/img/大数据/ch3-Learning-Based-Hashing-步骤.png" srcset="/img/loading.gif" lazyload /></p>
<h2 id="ch4-sampling">Ch4 Sampling</h2>
<h3 id="inverse-transform-sampling逆采样变换">Inverse Transform
Sampling(逆采样变换)</h3>
<p>逆采样变换（Inverse Transform
Sampling）是伪随机数采样的一种基本方法。在已知任意概率分布的累积分布函数<span
class="math inline">\(CDF\)</span>时，可以通过<span
class="math inline">\(CDF\)</span>的逆函数来实现随机数的采样。</p>
<p>简单来说，假设<span
class="math inline">\(X\)</span>为一个连续随机变量，其概率密度函数为<span
class="math inline">\(PDF(X)\)</span>，累计分布函数为<span
class="math inline">\(CDF(X)\)</span>。这时候若想生成符合<span
class="math inline">\(X\)</span>分布的随机变量样本，只需在<span
class="math inline">\([0, 1]\)</span>范围内生成随机变量<span
class="math inline">\(x\)</span>,然后放入<span
class="math inline">\(CDF\)</span>的反函数中，即可得到符合<span
class="math inline">\(X\)</span>分布的随机变量样本。</p>
<p>方法：</p>
<p><img src="/img/大数据/ch4-CDF.png" srcset="/img/loading.gif" lazyload /></p>
<p>优点：</p>
<ul>
<li>简单</li>
<li>适用于任意分布</li>
</ul>
<p>缺点：</p>
<ul>
<li>逆函数不一定存在或很难确定逆函数</li>
</ul>
<h3 id="rejection-sampling拒绝采样">Rejection Sampling(拒绝采样)</h3>
<p>拒绝采样（Rejection
Sampling）是一种基本的随机数采样方法。它的基本思想是：对于一个难以采样的分布，我们可以找到一个容易采样的分布，使得容易采样的分布包含难以采样的分布，然后从容易采样的分布中采样，若采样的点在难以采样的分布中，则接受该点，否则拒绝该点。</p>
<p>方法：</p>
<p><img src="/img/大数据/ch4-Rejection-Sampling.png" srcset="/img/loading.gif" lazyload /></p>
<p>步骤：</p>
<p><img src="/img/大数据/ch4-Rejection-Sampling-步骤.png" srcset="/img/loading.gif" lazyload /></p>
<p>这就使得Proposal Distribution <span
class="math inline">\(q(x)\)</span>的支撑集（support）要大于目标分布<span
class="math inline">\(p(x)\)</span>的支撑集。所以<span
class="math inline">\(q(x)\)</span>的分布选择很重要。</p>
<h3 id="importance-sampling重要性采样">Importance
Sampling(重要性采样)</h3>
<p>重要性采样（Importance Sampling）与Rejection
Sampling(拒绝采样)的区别在于，重要性采样不会拒绝采样的点，而是对采样的点赋予一个权重，使得采样的点更多地来自于目标分布。</p>
<h3 id="importance-sampling和rejection-sampling的区别">Importance
Sampling和Rejection Sampling的区别</h3>
<ul>
<li>RS的实例有一个相同的权重，只有部分的实例会被保留</li>
<li>IS的实例有不同的权重，所有的实例都会被保留</li>
<li>IS对对proposal distribution的选择更不敏感</li>
</ul>
<h3 id="markov-chain-monte-carlomcmc">Markov Chain Monte
Carlo(MCMC)</h3>
<p>马尔可夫链蒙特卡洛（Markov Chain Monte
Carlo，MCMC）是一种基于马尔可夫链的随机采样方法。它的基本思想是：对于一个难以采样的分布，我们可以构造一个马尔可夫链，使得该马尔可夫链的平稳分布为该难以采样的分布，然后从该马尔可夫链中采样，得到的样本服从该难以采样的分布。</p>
<p><strong>蒙特卡洛法</strong>：</p>
<p>蒙特卡洛法（Monte Carlo
Method）是一种基于随机数的数值计算方法。它的基本思想是：对于一个难以计算的问题，我们可以构造一个概率分布，使得该概率分布的期望为该问题的解，然后从该概率分布中采样，得到的样本的平均值即为该问题的解。</p>
<h4 id="detailed-balance-condition细致平衡条件">Detailed Balance
Condition(细致平衡条件)</h4>
<p>细致平衡条件（Detailed Balance
Condition）是马尔可夫链平稳分布的一个必要条件。它的基本思想是：对于一个马尔可夫链，若该马尔可夫链的平稳分布为<span
class="math inline">\(\pi(x)\)</span>，则该马尔可夫链的任意两个状态<span
class="math inline">\(x\)</span>和<span
class="math inline">\(y\)</span>满足：</p>
<p><span class="math display">\[\pi(x)P(x, y) = \pi(y)P(y,
x)\]</span></p>
<p>其中<span class="math inline">\(P(x, y)\)</span>为从状态<span
class="math inline">\(x\)</span>转移到状态<span
class="math inline">\(y\)</span>的概率,<span
class="math inline">\(\pi(x)\)</span>为状态<span
class="math inline">\(x\)</span>的概率。那么<span
class="math inline">\(\pi(x)\)</span>就是该马尔可夫链的平稳分布。</p>
<h4 id="the-procedure-of-mcmcmcmc的流程">The Procedure of
MCMC(MCMC的流程)</h4>
<p><img src="img/大数据/ch4-MCMC流程.png" srcset="/img/loading.gif" lazyload /></p>
<h3 id="metropolis-hastings-algorithmmh算法">Metropolis-Hastings
Algorithm(MH算法)</h3>
<p>由于MCMC采样有收敛太慢的问题,所以在MCMC的基础之上进行改进，引出MH算法。</p>
<p><img src="img/大数据/ch4-MH算法流程.png" srcset="/img/loading.gif" lazyload /></p>
<p>MH算法的具体流程如下：</p>
<p><img src="img/大数据/ch4-MH算法具体流程.png" srcset="/img/loading.gif" lazyload /></p>
<p>一般来说M-H采样算法较MCMC算法应用更广泛，然而在大数据时代，M-H算法面临着两个问题：</p>
<ol type="1">
<li>在高维时的计算量很大，算法效率很低，同时存在拒绝转移的问题，也会加大计算量</li>
<li>由于特征维度大，很多时候我们甚至很难求出目标的各特征维度联合分布，但是可以方便求出各个特征之间的条件概率分布（因此就思考是否能只知道条件概率分布的情况下进行采样）。</li>
</ol>
<h3 id="gibbs-sampling">Gibbs Sampling</h3>
<p>Gibbs Sampling是MH算法的一种特殊情况。它的基本思想是：</p>
<p><img src="img/大数据/ch4-Gibbs.png" srcset="/img/loading.gif" lazyload /> <img
src="img/大数据/ch4-Gibbs-1.png" srcset="/img/loading.gif" lazyload /></p>
<p>因此可以得出在二维的情况下Gibbs采样算法的流程如下：</p>
<p><img src="img/大数据/ch4-Gibbs-二维流程.png" srcset="/img/loading.gif" lazyload /></p>
<p>而在多维的情况下，比如一个n维的概率分布<span
class="math inline">\(π(x_1, x_2, ...x_n)\)</span>，我们可以通过在<span
class="math inline">\(n\)</span>个坐标轴上轮换采样，来得到新的样本。对于轮换到的任意一个坐标轴<span
class="math inline">\(x_i\)</span>上的转移，马尔科夫链的状态转移概率为<span
class="math inline">\(P(x_i|x_1, x_2, ..., x_{i−1}, x_{i+1}, ...,
x_n)\)</span>，即固定<span
class="math inline">\(n−1\)</span>个坐标轴，在某一个坐标轴上移动。而在多维的情况下Gibbs采样算法的流程如下：</p>
<p><img src="img/大数据/ch4-Gibbs-多维流程.png" srcset="/img/loading.gif" lazyload /></p>
<h3 id="gibbs-sampling和mh算法的联系与区别">Gibbs
Sampling和MH算法的联系与区别</h3>
<ul>
<li>Gibbs Sampling和MH都是MCMC</li>
<li>Acceptance Ratio:
<ul>
<li>Gibbs Sampling: <span class="math inline">\(1\)</span></li>
<li>MH: <span class="math inline">\(\frac{\pi(y)q(y, x)}{\pi(x)q(x, y)}
&lt; 1\)</span></li>
</ul></li>
<li>MH不需要知道条件概率，而Gibbs Sampling需要知道条件概率</li>
</ul>
<h3 id="reservoir-sampling">Reservoir sampling</h3>
<p>Reservoir
sampling是一种随机采样算法，它的基本思想是：对于一个数据流，我们希望从中随机采样出<span
class="math inline">\(k\)</span>个样本，但是我们不知道数据流的长度，也就是说我们不知道<span
class="math inline">\(k\)</span>的大小。具体流程如下：</p>
<p><img src="img/大数据/ch4-Reservoir.png" srcset="/img/loading.gif" lazyload /></p>
<h2 id="ch5-data-stream-mining">Ch5 Data Stream Mining</h2>
<h3 id="data-stream">Data Stream</h3>
<h4 id="properity-of-data-stream">Properity of Data Stream</h4>
<p>数据流（Data Stream）是一种连续不断的数据，它的特点是：</p>
<ul>
<li>One by One(逐个到达)</li>
<li>Potentially Unbounded(无界)</li>
<li><strong>Concept Drift(概念漂移)</strong></li>
</ul>
<h4 id="conncept-drift概念漂移">Conncept Drift(概念漂移)</h4>
<p>概念漂移（Concept
Drift）是指数据流中的数据分布随着时间的推移而发生变化的现象。概念漂移分为两种类型：</p>
<ul>
<li>Real concept drift (真实概念漂移)</li>
<li>Virtual concept drift (虚假概念漂移)</li>
</ul>
<h4 id="concept-drift-detection概念漂移检测">Concept Drift
Detection(概念漂移检测)</h4>
<ol type="1">
<li>Distribution-based detector(基于分布的检测器)</li>
</ol>
<p>监测两个固定或可变化的数据窗口之间的数据分布变化，如果数据分布发生变化，则认为发生了概念漂移。方法很简单：只要<span
class="math inline">\(W\)</span>的两个足够大的子窗口<span
class="math inline">\(W_1\)</span>和<span
class="math inline">\(W_2\)</span>的数据分布不同，就认为发生了概念漂移，这时较旧的窗口就被放弃。</p>
<p><strong>Adaptive Windowing(ADWIN)</strong>:</p>
<p>ADWIN 的思想是从时间窗口 <span class="math inline">\(W\)</span>
开始，在上下文没有明显变化时动态增大窗口 <span
class="math inline">\(W\)</span>，并在检测到变化时将其缩小。
该算法试图找到显示不同平均值的 <span class="math inline">\(W -
w_0\)</span> 和 <span class="math inline">\(w_1\)</span> 的两个子窗口。
这意味着窗口的旧部分 <span class="math inline">\(- w_0\)</span>
是基于与实际不同的数据分布，因此被删除。</p>
<p><strong>draw back(缺点)</strong>：</p>
<ul>
<li>计算成本高</li>
<li>模型复杂度高</li>
<li>无法检测全局漂移</li>
<li>对数据集的依赖性高</li>
</ul>
<ol start="2" type="1">
<li>Error-rate based detector(基于错误率的检测器)</li>
</ol>
<p>根据分类性能的变化来捕捉概念的漂移,如果分类器的错误率超过了某个阈值，则认为发生了概念漂移。</p>
<p>DDM算法：</p>
<p>DDM算法的基本思想是：在数据流中，如果某个时间点的错误率比之前的错误率大很多，则认为发生了概念漂移。确定错误率的变化是否显著的方法如下公式：</p>
<p><span class="math display">\[p_i + s_i \ge p_{min} + 3 \times
s_{min}\]</span></p>
<p>误差率是指观察到错误的概率<span
class="math inline">\(p_i\)</span>，其标准差为<span
class="math inline">\(s_i = sqrt(p_i (1 - p_i) / i)\)</span></p>
<p><strong>draw back(缺点)</strong>：</p>
<ul>
<li>对噪声敏感</li>
<li>对异常数据分布假设的依赖性高</li>
<li>受到数据流非稳态性的影响</li>
<li>受到样本的影响</li>
<li>无法处理高维数据</li>
</ul>
<h4 id="data-stream面临的挑战">Data Stream面临的挑战</h4>
<ul>
<li>Infinite Length(无限长度)</li>
<li>Evolving Nature(不断变化的数据)</li>
</ul>
<h3 id="data-stream-clissification数据流分类">Data Stream
Clissification(数据流分类)</h3>
<p>流程：</p>
<ul>
<li>从数据流中读取下一个可用数据(要求1)</li>
<li>用读取的数据更新分类器，并且这样做不回超过对它设置的内存限制(要求2)，并尽可能快地完成（要求3）</li>
<li>算法已经学习了足够的数据，以便在新数据上进行分类(要求4)</li>
</ul>
<p><strong>典型算法</strong>：</p>
<ul>
<li>VFDT(very fast decision tree, KDD'00)</li>
<li>CVFDT(Concept-adapting very fast decision tree, KDD'01)</li>
<li>SyncStream(同步流算法, KDD'14)</li>
</ul>
<h4 id="vfdt">VFDT</h4>
<p>Hoeffding树是一种基于决策树学习的数据流分类算法，在处理数据流时，可以保证挖掘效率的同时，达到对数据流一些必要操作的要求。该算法简单的对数据流中的每个样本检查一次，并逐步生成一颗决策树，而在这些样本更新完决策树之后无需进行存储。
在内存中只需维护决策树信息，因为在决策树的叶结点中存储着决策树扩展所必须的统计信息，并且在处理训练数据集时，可以用决策树中的信息进行预测。</p>
<p>VFDT（very fast decision tree）是基于Hoeffding
tree改进的算法和系统，它和Hoeffding
tree算法相似之处在于都是根据<strong>Hoeffding
不等式</strong>来决定决策节点的最佳属性从而建立决策树模型。</p>
<p>Hoeffding 不等式:</p>
<p>Hoeffding不等式适用于有界的随机变量。设有两两独立的一系列随机变量<span
class="math inline">\(X_1, X_2, ..., X_n\)</span>，且<span
class="math inline">\(X_i\)</span>的取值范围是<span
class="math inline">\([a_i, b_i]\)</span>，这<span
class="math inline">\(n\)</span>个随机变量的经验期望<span
class="math inline">\(\bar{X}=\frac{X_1 + \dots +
X_n}{n}\)</span>满足以下不等式：</p>
<p><span class="math display">\[P(|\bar{X} - E(\bar{X})| \ge \epsilon)
\ge \exp(-\frac{2n^2\epsilon^2}{\sum_{i=1}^n(b_i - a_i)^2})\]</span></p>
<p><span class="math display">\[P(|\bar{X} - E(\bar{X})| \ge \epsilon)
\le 2 \exp(-\frac{2n^2\epsilon^2}{\sum_{i=1}^n(b_i -
a_i)^2})\]</span></p>
<p>其中<span class="math inline">\(E(\bar{X})\)</span>是<span
class="math inline">\(\bar{X}\)</span>的期望，<span
class="math inline">\(\epsilon\)</span>是一个正数。</p>
<p>VFDT系统解决了Hoeffding
tree算法没有提到的实际问题，就是当两个属性的信息熵差不多时，这个时候就会发生两个属性之间的权衡。这是系统需要花费大量的时间和空间，利用更多的样本来决定选择哪个属性为最佳的决策节点的属性，而这显然是浪费的。</p>
<p>VFDT算法相较于Hoeffding Tress算法的改进：</p>
<ul>
<li>提供了一个用户定义的阈值<span class="math inline">\(τ\)</span>
用来解决“两个属性的信息熵差不多时的博弈”。当信息熵差值小于某个阈值时，即可判定其为决策节点属性。</li>
<li>允许设定节点的最小样本个数值<span
class="math inline">\(n_{min}\)</span>，在用户能够承受的置信度下，让用户设定每个节点最小的样本数将有效的减少样本信息熵<span
class="math inline">\(G\)</span>的计算而消耗的时间复杂度。</li>
<li>提供重新扫描数据集和二次抽样的功能，并且在数据流中的样本数减少时，决策树的精度也会无限逼近于读取所有样本建立决策树的精度。</li>
</ul>
<p><strong>以下是 VFDT 算法的基本流程</strong>：</p>
<ol type="1">
<li><p>构建决策树：对于一个分类问题，首先需要构建一颗决策树。该决策树会被
VFDT 算法不断地更新和重新构建。</p></li>
<li><p>建立示例集：随机选择一些实例作为示例集。</p></li>
<li><p>计算初始统计信息：对于示例集中的每个实例，计算它们分类结果的概率分布。</p></li>
<li><p>增量统计每个实例：对于新增加的每个实例，将其加入当前的示例集，并更新分类结果的概率分布。</p></li>
<li><p>检查增量误差：计算每个分类器的误差，并选择一个误差最小的分类器来更新决策树。</p></li>
<li><p>执行更新：将当前分类器放到决策树上对应的位置，并更新决策树。</p></li>
</ol>
<p>重复步骤 4-6，直到决策树满足一定条件。</p>
<p>VFDT
算法是一种增量式建树算法，它通过不断更新决策树的方法来尽可能地减小误差。这种算法的好处是，可以随时加入新的数据，更新模型，同时不需要重新训练整个模型。但是，VFDT
算法也存在一些缺点，比如计算复杂度较高，对异常数据较为敏感等。</p>
<h5 id="vfdt的优缺点">VFDT的优缺点</h5>
<p><strong>优点</strong>：</p>
<ul>
<li>Scales better than traditional methods(比传统方法更好)
<ul>
<li>Sublinear with sampling(子线性采样)</li>
<li>Very small memory utilization(非常小的内存使用率)</li>
</ul></li>
<li>Incremental(增量学习)
<ul>
<li>Make class predictions in parallel(并行预测分类)</li>
<li>New examples are added as they come(新的样本随着到来而添加)</li>
</ul></li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>Could spend a lot of time with ties(可能会花费很多时间)</li>
<li>Memory used with tree expansion(内存使用率随着树的扩展而增加)</li>
<li>Number of candidate attributes(候选属性的数量大)</li>
</ul>
<h4
id="cvfdtconcept-adapting-very-fast-decision-tree">CVFDT(Concept-adapting
very fast decision tree)</h4>
<p>CVFDT是VFDT的改进版，它保持了VFDT的精度和速度，VFDT
算法假设所分析处理的数据流是平稳分布的，所以应对数据流中概念变化时采用的是单一的决策树模型，这就导致VFDT的决策树模型不能及时反映数据流随时间变化的趋势。</p>
<p>另外VFDT也没有处理连续值属性的问题。因为在CVFDT
中滑动窗口的引入，过时的样本都被删除，所以 CVFDT 树比 VFDT 树要小很多。
CVFDT根据滑动窗口中的数据流样本来持续检测旧的决策树的有效性从而保证建立模型与概念漂移同步。</p>
<p>CVFDT算法对VFDT算法的改进如下：</p>
<ul>
<li><strong>CVFDT算法解决了VFDT算法不能处理数据流中概念漂移的问题</strong>。通过在VFDT算法基础上添加滑动窗口使得建立决策树模型的数据流能够不断实现更新，保证在概念漂移的数据流中保持模型的准确率。</li>
<li>对于每个节点包括根节点都有相应的ID。样本遍历每个节点时不仅会在节点处保存其样本的属性信息，同时窗口中的样本也会保存其遍历过的节点信息。当样本滑出窗口时，该样本所经历过的节点统计值将依次减一。</li>
<li>CVFDT还为每个决策节点设置备选子树，周期性的检测每个决策节点的准确率从而决定替代子树是否替换当前的决策节点，从而也有效的提高了决策树模型的准确率。</li>
</ul>
<p>CVFDT算法流程如下：</p>
<p><img src="/img/大数据/ch4-CVFDT-流程.png" srcset="/img/loading.gif" lazyload /></p>
<p>CVFDT算法采用增量的方式训练决策树，解决了VFDT算法不能处理连续属性的问题，并且在处理大规模数据时效率更高。</p>
<h4 id="syncstream">SyncStream</h4>
<p>KNN style</p>
<h4 id="open-set-problem">Open-set problem</h4>
<ul>
<li>Novel class Dection(Extreme Value Theory,EVT)</li>
</ul>
<p>EVT 中心思想是概率分布，可给出事件发生概率的数学公式。</p>
<ul>
<li>Continued learning(Elastic Weight Consolidation,EWC)</li>
</ul>
<p>EWC的基本思想：模型中的一些参数对前面的任务很重要。只改变不重要的参数</p>
<p>Gradient Episodic Memory
(GEM)的基本思想:限制梯度的方向来改善之前的工作</p>
<ul>
<li>Class-incremental learning</li>
</ul>
<p>问题：</p>
<ol type="1">
<li>怎么平衡新旧类的样本</li>
<li>怎么平衡新旧类的样本的重要性</li>
<li>怎么提取榜样样本(exemplars examples)</li>
</ol>
<p><strong>Knowledge Distillation(知识蒸馏)</strong>:</p>
<p><strong>Weight Aligning(权重对齐)</strong>:
通过对齐权重来减少模型的参数数量</p>
<h3 id="data-stream-clustering流聚类">Data Stream
Clustering(流聚类)</h3>
<h4 id="framework">Framework</h4>
<p>有两个阶段</p>
<ol type="1">
<li>online Data
abstraction(数据抽象):将数据归纳为具有内存效率的数据结构</li>
<li>offline clustering(离线聚类):使用聚类算法来寻找数据类别</li>
</ol>
<p>online阶段首先根据K-mens算法生成p个初始的聚类中心(micro
clusters)，并为每一个簇提供一个独一无二的ID，其中P是大于具体的聚类数目但是要远远小于具体数据点的个数。</p>
<p>对于每一个到来的数据点，要么被现有的微集群吸收（是否在一个集群的最大边界内-均值根偏差RMS），要么自己成立一个集群。但是数据点不属于现有的集群有两种情况，一种是该点是一个异常点，第二种是该点是一个新集群的起始点。</p>
<p>那么如果要新建立一个集群的话，就需要将已有的集群删除一个或者合并两个相似的集群。若要删除一个集群的话，首先判断删除该集群是否会有不良的影响，所以根据该集群的数据点的时间戳信息来判断，如果该簇的时间戳不满足设定的阈值，将其删除。合并集群的话，将两个最近的集群进行合并。</p>
<p>流聚类算法：</p>
<p><img src="/img/大数据/ch5-流聚类算法.png" srcset="/img/loading.gif" lazyload /></p>
<ol start="2" type="1">
<li>offline clustering(离线聚类)</li>
</ol>
<p>根据用户输入的需要查看的时间，从特征金字塔中取出两个时间段的汇总信息，相减之后就得到用户所需时间段的近似数据集，在此基础上进行聚类即可。</p>
<h4 id="online-data-abstraction">online Data abstraction</h4>
<ol type="1">
<li>A Micro-Cluster is a set of individual data points that are close to
each other and will be treated as a single unit in further offline
Macro-clustering.(微集群是一组彼此接近的单个数据点，将在进一步的离线宏聚类中作为单个单元处理。)</li>
<li>动态选择short-term和long-term的代表性example，代表性高的保留，代表性低的删除，没有，若代表性没有改变，就进行statistic
summary(摘要统计)</li>
<li>簇特征的属性</li>
</ol>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>大数据期末复习</div>
      <div>https://gstarmin.github.io/2023/06/02/大数据期末复习/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Starmin</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年6月2日</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>更新于</div>
          <div>2023年6月4日</div>
        </div>
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/06/01/%E5%89%8D%E7%BC%80%E5%92%8C%E6%95%B0%E7%BB%84%E4%B8%8E%E5%B7%AE%E5%88%86%E6%95%B0%E7%BB%84/" title="前缀和数组与差分数组">
                        <span class="hidden-mobile">前缀和数组与差分数组</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
